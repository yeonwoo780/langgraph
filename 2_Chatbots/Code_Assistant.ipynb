{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code generation with RAG and self-correction\n",
    "![image](../image/2_chatbots/Code_Assisant/main.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yeonwoo/code/study'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Sort the list based on the URLs and get the text\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code(prefix='이 문서에서는 LCEL을 사용하여 RAG(Retrieval Augmented Generation) 체인을 구축하는 방법을 설명합니다. RAG 체인은 외부 지식 베이스를 사용하여 언어 모델의 성능을 향상시키는 기술입니다. 이를 위해서는 벡터 저장소와 검색기를 설정하고, 이를 통해 검색된 정보를 기반으로 언어 모델이 응답을 생성하도록 구성해야 합니다.', imports=\"import { VectorStore, Retriever, LLM, RAGChain } from 'langchain';\", code=\"// 벡터 저장소를 초기화합니다.\\nconst vectorStore = new VectorStore({\\n  // 벡터 저장소 설정\\n});\\n\\n// 검색기를 초기화합니다.\\nconst retriever = new Retriever({\\n  vectorStore,\\n  // 검색기 설정\\n});\\n\\n// 언어 모델을 초기화합니다.\\nconst llm = new LLM({\\n  // 언어 모델 설정\\n});\\n\\n// RAG 체인을 구성합니다.\\nconst ragChain = new RAGChain({\\n  retriever,\\n  llm,\\n  // 추가 설정\\n});\\n\\n// RAG 체인을 실행하여 결과를 얻습니다.\\nconst result = ragChain.run({\\n  query: '질문을 입력하세요',\\n});\\n\\nconsole.log(result);\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "### OpenAI\n",
    "\n",
    "# Grader prompt\n",
    "prompt = \"\"\"당신은 LangChain 표현언어인 LCEL에 대한 전문성을 갖춘 코딩 어시스턴트입니다.\\n\n",
    "다음은 전체 LCEL 문서 세트입니다: \\n ------- \\n {context} \\n ------- \\n 사용자에게 답변\n",
    "위에 제공된 문서를 기반으로 한 질문입니다. 제공한 코드가 실행될 수 있는지 확인하십시오. \\n\n",
    "필요한 모든 가져오기 및 변수가 정의되어 있습니다. 코드 솔루션에 대한 설명을 사용하여 답변을 구성하세요. \\n\n",
    "그런 다음 수입품을 나열하십시오. 마지막으로 작동하는 코드 블록을 나열합니다. 사용자 질문은 다음과 같습니다.\n",
    "\"\"\"\n",
    "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            f\"{prompt}\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Data model\n",
    "class code(BaseModel):\n",
    "    \"\"\"Schema for code solutions to questions about LCEL.\"\"\"\n",
    "\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-2024-08-06\", temperature=0)\n",
    "code_gen_chain_oai = code_gen_prompt | llm.with_structured_output(code)\n",
    "question = \"How do I build a RAG chain in LCEL? please say korean\"\n",
    "solution = code_gen_chain_oai.invoke(\n",
    "    {\"context\": concatenated_content, \"messages\": [(\"user\", question)]}\n",
    ")\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "### Anthropic\n",
    "\n",
    "# Prompt to enforce tool use\n",
    "code_gen_prompt_claude = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"<instructions> You are a coding assistant with expertise in LCEL, LangChain expression language. \\n \n",
    "    Here is the LCEL documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user  question based on the \\n \n",
    "    above provided documentation. Ensure any code you provide can be executed with all required imports and variables \\n\n",
    "    defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the functioning code block. \\n\n",
    "    Invoke the code tool to structure the output correctly. </instructions> \\n Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-2024-08-06\", temperature=0,\n",
    "    default_headers={\"anthropic-beta\": \"tools-2024-04-04\"},\n",
    ")\n",
    "structured_llm_claude = llm.with_structured_output(code, include_raw=True)\n",
    "\n",
    "# Optional: Check for errors in case tool use is flaky\n",
    "def check_claude_output(tool_output):\n",
    "    \"\"\"Check for parse error or failure to call the tool\"\"\"\n",
    "\n",
    "    # Error with parsing\n",
    "    if tool_output[\"parsing_error\"]:\n",
    "        # Report back output and parsing errors\n",
    "        print(\"Parsing error!\")\n",
    "        raw_output = str(tool_output[\"raw\"].content)\n",
    "        error = tool_output[\"parsing_error\"]\n",
    "        raise ValueError(\n",
    "            f\"Error parsing your output! Be sure to invoke the tool. Output: {raw_output}. \\n Parse error: {error}\"\n",
    "        )\n",
    "\n",
    "    # Tool was not invoked\n",
    "    elif not tool_output[\"parsed\"]:\n",
    "        print(\"Failed to invoke tool!\")\n",
    "        raise ValueError(\n",
    "            \"You did not use the provided tool! Be sure to invoke the tool to structure the output.\"\n",
    "        )\n",
    "    return tool_output\n",
    "\n",
    "\n",
    "# Chain with output check\n",
    "code_chain_claude_raw = (\n",
    "    code_gen_prompt_claude | structured_llm_claude | check_claude_output\n",
    ")\n",
    "\n",
    "\n",
    "def insert_errors(inputs):\n",
    "    \"\"\"Insert errors for tool parsing in the messages\"\"\"\n",
    "\n",
    "    # Get errors\n",
    "    error = inputs[\"error\"]\n",
    "    messages = inputs[\"messages\"]\n",
    "    messages += [\n",
    "        (\n",
    "            \"assistant\",\n",
    "            f\"Retry. You are required to fix the parsing errors: {error} \\n\\n You must invoke the provided tool.\",\n",
    "        )\n",
    "    ]\n",
    "    return {\n",
    "        \"messages\": messages,\n",
    "        \"context\": inputs[\"context\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# This will be run as a fallback chain\n",
    "fallback_chain = insert_errors | code_chain_claude_raw\n",
    "N = 3  # Max re-tries\n",
    "code_gen_chain_re_try = code_chain_claude_raw.with_fallbacks(\n",
    "    fallbacks=[fallback_chain] * N, exception_key=\"error\"\n",
    ")\n",
    "\n",
    "\n",
    "def parse_output(solution):\n",
    "    \"\"\"When we add 'include_raw=True' to structured output,\n",
    "    it will return a dict w 'raw', 'parsed', 'parsing_error'.\"\"\"\n",
    "\n",
    "    return solution[\"parsed\"]\n",
    "\n",
    "\n",
    "# Optional: With re-try to correct for failure to invoke tool\n",
    "code_gen_chain = code_gen_chain_re_try | parse_output\n",
    "\n",
    "# No re-try\n",
    "code_gen_chain = code_gen_prompt_claude | structured_llm_claude | parse_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code(prefix='이 코드는 LCEL에서 RAG 체인을 구축하는 방법을 보여줍니다. Retrieval Augmented Generation (RAG)은 외부 지식 베이스와 결합하여 언어 모델을 향상시키는 기술입니다.', imports=\"import { RAGChain } from 'langchain';\\nimport { VectorStore } from 'langchain/vectorstores';\\nimport { Retriever } from 'langchain/retrievers';\", code=\"// 벡터 스토어와 리트리버를 설정합니다.\\nconst vectorStore = new VectorStore({\\n  // 벡터 스토어 설정\\n});\\n\\nconst retriever = new Retriever({\\n  vectorStore: vectorStore,\\n  // 리트리버 설정\\n});\\n\\n// RAG 체인을 생성합니다.\\nconst ragChain = new RAGChain({\\n  retriever: retriever,\\n  // 추가 설정\\n});\\n\\n// RAG 체인을 사용하여 쿼리를 실행합니다.\\nconst query = '질문을 입력하세요';\\nconst result = await ragChain.run(query);\\nconsole.log(result);\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "question = \"How do I build a RAG chain in LCEL? please say korean\"\n",
    "solution = code_gen_chain.invoke(\n",
    "    {\"context\": concatenated_content, \"messages\": [(\"user\", question)]}\n",
    ")\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        error : Binary flag for control flow to indicate whether test error was tripped\n",
    "        messages : With user question, error messages, reasoning\n",
    "        generation : Code solution\n",
    "        iterations : Number of tries\n",
    "    \"\"\"\n",
    "\n",
    "    error: str\n",
    "    messages: List\n",
    "    generation: str\n",
    "    iterations: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameter\n",
    "\n",
    "# Max tries\n",
    "max_iterations = 3\n",
    "# Reflect\n",
    "# flag = 'reflect'\n",
    "flag = \"do not reflect\"\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate a code solution\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    error = state[\"error\"]\n",
    "\n",
    "    # We have been routed back to generation with an error\n",
    "    if error == \"yes\":\n",
    "        messages += [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:\",\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Solution\n",
    "    code_solution = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": messages}\n",
    "    )\n",
    "    messages += [\n",
    "        (\n",
    "            \"assistant\",\n",
    "            f\"{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Increment\n",
    "    iterations = iterations + 1\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "\n",
    "def code_check(state: GraphState):\n",
    "    \"\"\"\n",
    "    Check code\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, error\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECKING CODE---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    # Get solution components\n",
    "    imports = code_solution.imports\n",
    "    code = code_solution.code\n",
    "\n",
    "    # Check imports\n",
    "    try:\n",
    "        exec(imports)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE IMPORT CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # Check execution\n",
    "    try:\n",
    "        exec(imports + \"\\n\" + code)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE BLOCK CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # No errors\n",
    "    print(\"---NO CODE TEST FAILURES---\")\n",
    "    return {\n",
    "        \"generation\": code_solution,\n",
    "        \"messages\": messages,\n",
    "        \"iterations\": iterations,\n",
    "        \"error\": \"no\",\n",
    "    }\n",
    "\n",
    "\n",
    "def reflect(state: GraphState):\n",
    "    \"\"\"\n",
    "    Reflect on errors\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "\n",
    "    # Prompt reflection\n",
    "\n",
    "    # Add reflection\n",
    "    reflections = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": messages}\n",
    "    )\n",
    "    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_finish(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines whether to finish.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    error = state[\"error\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    if error == \"no\" or iterations == max_iterations:\n",
    "        print(\"---DECISION: FINISH---\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "        if flag == \"reflect\":\n",
    "            return \"reflect\"\n",
    "        else:\n",
    "            return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"generate\", generate)  # generation solution\n",
    "workflow.add_node(\"check_code\", code_check)  # check code\n",
    "workflow.add_node(\"reflect\", reflect)  # reflect\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"generate\")\n",
    "workflow.add_edge(\"generate\", \"check_code\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_code\",\n",
    "    decide_to_finish,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"reflect\": \"reflect\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"reflect\", \"generate\")\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: FINISH---\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt? prease say korean\"\n",
    "solution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code(prefix='이 코드는 문자열을 직접 Runnable에 전달하고 이를 사용하여 프롬프트에 필요한 입력을 구성하는 방법을 보여줍니다.', imports='from langchain_core.prompts import PromptTemplate\\nfrom langchain_core.runnables import Runnable', code='# Define a simple prompt template\\nprompt_template = PromptTemplate(template=\"Translate the following text to English: {text}\")\\n\\n# Create a Runnable with the prompt template\\ndef create_runnable(input_text):\\n    # Construct the input for the prompt\\n    prompt_input = {\\'text\\': input_text}\\n    \\n    # Create a Runnable instance\\n    runnable = Runnable(prompt_template, prompt_input)\\n    \\n    # Execute the Runnable\\n    result = runnable.invoke()\\n    return result\\n\\n# Example usage\\ninput_string = \"안녕하세요\"\\noutput = create_runnable(input_string)\\nprint(output)')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution[\"generation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langsmith\n",
    "\n",
    "client = langsmith.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the dataset to your tenant to use it\n",
    "try:\n",
    "    public_dataset = (\n",
    "        \"https://smith.langchain.com/public/326674a6-62bd-462d-88ae-eea49d503f9d/d\"\n",
    "    )\n",
    "    client.clone_public_dataset(public_dataset)\n",
    "except:\n",
    "    print(\"Please setup LangSmith\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "\n",
    "def check_import(run: Run, example: Example) -> dict:\n",
    "    imports = run.outputs.get(\"imports\")\n",
    "    try:\n",
    "        exec(imports)\n",
    "        return {\"key\": \"import_check\", \"score\": 1}\n",
    "    except Exception:\n",
    "        return {\"key\": \"import_check\", \"score\": 0}\n",
    "\n",
    "\n",
    "def check_execution(run: Run, example: Example) -> dict:\n",
    "    imports = run.outputs.get(\"imports\")\n",
    "    code = run.outputs.get(\"code\")\n",
    "    try:\n",
    "        exec(imports + \"\\n\" + code)\n",
    "        return {\"key\": \"code_execution_check\", \"score\": 1}\n",
    "    except Exception:\n",
    "        return {\"key\": \"code_execution_check\", \"score\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_base_case(example: dict):\n",
    "    \"\"\"Context stuffing\"\"\"\n",
    "    solution = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": [(\"user\", example[\"question\"])]}\n",
    "    )\n",
    "    return {\"imports\": solution.imports, \"code\": solution.code}\n",
    "\n",
    "\n",
    "def predict_langgraph(example: dict):\n",
    "    \"\"\"LangGraph\"\"\"\n",
    "    graph = app.invoke(\n",
    "        {\"messages\": [(\"user\", example[\"question\"])], \"iterations\": 0, \"error\": \"\"}\n",
    "    )\n",
    "    solution = graph[\"generation\"]\n",
    "    return {\"imports\": solution.imports, \"code\": solution.code}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Evaluator\n",
    "code_evalulator = [check_import, check_execution]\n",
    "\n",
    "# Dataset\n",
    "dataset_name = \"lcel-teacher-eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yeonwoo/miniforge3/envs/langgraph/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-without-langgraph-client=<openai.resources.chat.completions.Completions object at 0x7fd0637f43d0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fd06318f190> root_client=<openai.OpenAI object at 0x7fd06331d9d0> root_async_client=<openai.AsyncOpenAI object at 0x7fd063142690> model_name='gpt-4o-2024-08-06' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********') default_headers={'anthropic-beta': 'tools-2024-04-04'}-4ca32873' at:\n",
      "https://smith.langchain.com/o/0708e8be-8469-58eb-8179-f15e52e09b25/datasets/ea83dffd-1b68-441c-a6c1-62f28b7b09a0/compare?selectedSessions=ccda3473-53ce-4dbd-abf2-5d3ffbf7adf0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:25,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using itemgetter: What is LCEL?\n",
      "Using lambda: What is LCEL?\n",
      "Using get method: What is LCEL?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:45,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run base case\n",
    "try:\n",
    "    experiment_results_ = evaluate(\n",
    "        predict_base_case,\n",
    "        data=dataset_name,\n",
    "        evaluators=code_evalulator,\n",
    "        experiment_prefix=f\"test-without-langgraph-{llm}\",\n",
    "        max_concurrency=2,\n",
    "        metadata={\n",
    "            \"llm\": llm,\n",
    "        },\n",
    "    )\n",
    "except:\n",
    "    print(\"Please setup LangSmith\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-with-langgraph-client=<openai.resources.chat.completions.Completions object at 0x7fd0637f43d0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fd06318f190> root_client=<openai.OpenAI object at 0x7fd06331d9d0> root_async_client=<openai.AsyncOpenAI object at 0x7fd063142690> model_name='gpt-4o-2024-08-06' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********') default_headers={'anthropic-beta': 'tools-2024-04-04'}-do not reflect-d3dda7e8' at:\n",
      "https://smith.langchain.com/o/0708e8be-8469-58eb-8179-f15e52e09b25/datasets/ea83dffd-1b68-441c-a6c1-62f28b7b09a0/compare?selectedSessions=f0ed8bc4-5ad7-4eb7-9e6f-2336142a1deb\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION------GENERATING CODE SOLUTION---\n",
      "\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "{'a': 1, 'b': 2, 'c': 3}\n",
      "---NO CODE TEST FAILURES---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n",
      "{'a': 1, 'b': 2, 'c': 3}\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:12, 12.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:14,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:23,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:25,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECKING CODE---\n",
      "{'output': {'num': 1, 'num2': 2}}\n",
      "---NO CODE TEST FAILURES---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n",
      "{'output': {'num': 1, 'num2': 2}}\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:36,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECKING CODE---\n",
      "---NO CODE TEST FAILURES---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:37,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:48,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:51,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:03,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [01:07,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECKING CODE---\n",
      "Using itemgetter: What is LCEL?\n",
      "Using lambda: What is LCEL?\n",
      "Using get method: What is LCEL?\n",
      "---NO CODE TEST FAILURES---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n",
      "Using itemgetter: What is LCEL?\n",
      "Using lambda: What is LCEL?\n",
      "Using get method: What is LCEL?\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [01:13,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:18,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "<string>:13: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "<string>:12: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "<string>:20: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hello there! How can I assist you?\n",
      "---NO CODE TEST FAILURES---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:54,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECKING CODE---\n",
      "---CODE IMPORT CHECK: FAILED---\n",
      "---DECISION: FINISH---\n",
      "Please setup LangSmith\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run with langgraph\n",
    "try:\n",
    "    experiment_results = evaluate(\n",
    "        predict_langgraph,\n",
    "        data=dataset_name,\n",
    "        evaluators=code_evalulator,\n",
    "        experiment_prefix=f\"test-with-langgraph-{llm}-{flag}\",\n",
    "        max_concurrency=2,\n",
    "        metadata={\n",
    "            \"llm\": llm,\n",
    "            \"feedback\": flag,\n",
    "        },\n",
    "    )\n",
    "except:\n",
    "    print(\"Please setup LangSmith\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
