{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a9b6c8",
   "metadata": {},
   "source": [
    "### LLM컴파일러"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e68035",
   "metadata": {},
   "source": [
    "LLMCompiler는 DAG 내에서 열렬히 실행되는 작업에 의한 에이전트 작업의 실행 속도를 높이기 위해 설계된 에이전트 아키텍처. \n",
    "또한 LLM에 대한 호출 수를 줄임으로써 중복 토큰 사용 비용을 절감합니다. 아래는 계산 그래프의 개요.\n",
    "\n",
    "![image](../image/4_Agent/LLMCompiler1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309e7371",
   "metadata": {},
   "source": [
    "3가지 주요 구성요소가 있습니다.\n",
    "\n",
    "1. 플래너: 작업 DAG를 스트리밍합니다.\n",
    "2. 작업 가져오기 단위: 작업이 실행 가능하자마자 작업을 예약하고 실행합니다.\n",
    "3. Joiner: 사용자에게 응답하거나 두 번째 계획을 트리거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9684d146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aaea37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7973875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab44468",
   "metadata": {},
   "source": [
    "#### Math Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a48450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "import numexpr\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import StructuredTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "_MATH_DESCRIPTION = (\n",
    "    \"math(problem: str, context: Optional[list[str]]) -> float:\\n\"\n",
    "    \" - Solves the provided math problem.\\n\"\n",
    "    ' - `problem` can be either a simple math problem (e.g. \"1 + 3\") or a word problem (e.g. \"how many apples are there if there are 3 apples and 2 apples\").\\n'\n",
    "    \" - You cannot calculate multiple expressions in one call. For instance, `math('1 + 3, 2 + 4')` does not work. \"\n",
    "    \"If you need to calculate multiple expressions, you need to call them separately like `math('1 + 3')` and then `math('2 + 4')`\\n\"\n",
    "    \" - Minimize the number of `math` actions as much as possible. For instance, instead of calling \"\n",
    "    '2. math(\"what is the 10% of $1\") and then call 3. math(\"$1 + $2\"), '\n",
    "    'you MUST call 2. math(\"what is the 110% of $1\") instead, which will reduce the number of math actions.\\n'\n",
    "    # Context specific rules below\n",
    "    \" - You can optionally provide a list of strings as `context` to help the agent solve the problem. \"\n",
    "    \"If there are multiple contexts you need to answer the question, you can provide them as a list of strings.\\n\"\n",
    "    \" - `math` action will not see the output of the previous actions unless you provide it as `context`. \"\n",
    "    \"You MUST provide the output of the previous actions as `context` if you need to do math on it.\\n\"\n",
    "    \" - You MUST NEVER provide `search` type action's outputs as a variable in the `problem` argument. \"\n",
    "    \"This is because `search` returns a text blob that contains the information about the entity, not a number or value. \"\n",
    "    \"Therefore, when you need to provide an output of `search` action, you MUST provide it as a `context` argument to `math` action. \"\n",
    "    'For example, 1. search(\"Barack Obama\") and then 2. math(\"age of $1\") is NEVER allowed. '\n",
    "    'Use 2. math(\"age of Barack Obama\", context=[\"$1\"]) instead.\\n'\n",
    "    \" - When you ask a question about `context`, specify the units. \"\n",
    "    'For instance, \"what is xx in height?\" or \"what is xx in millions?\" instead of \"what is xx?\"\\n'\n",
    ")\n",
    "\n",
    "_SYSTEM_PROMPT = \"\"\"Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
    "\n",
    "Question: ${{Question with math problem.}}\n",
    "\n",
    "```\n",
    "${{single line mathematical expression that solves the problem}}\n",
    "```\n",
    "...numexpr.evaluate(text)...\n",
    "\n",
    "```\n",
    "${{single line mathematical expression that solves the problem}}\n",
    "```\n",
    "Answer: ${{Answer}}\n",
    "\n",
    "Begin.\n",
    "\n",
    "Question: What is 37593 * 67?\n",
    "ExecuteCode({{code: \"37593 * 67\"}})\n",
    "...numexpr.evaluate(\"37593 * 67\")...\n",
    "\n",
    "```\n",
    "2518731\n",
    "```\n",
    "\n",
    "Answer: 2518731\n",
    "\n",
    "Question: 37593^(1/5)\n",
    "ExecuteCode({{code: \"37593**(1/5)\"}})\n",
    "...numexpr.evaluate(\"37593**(1/5)\")...\n",
    "\n",
    "```\n",
    "8.222831614237718\n",
    "```\n",
    "\n",
    "Answer: 8.222831614237718\n",
    "\"\"\"\n",
    "\n",
    "_ADDITIONAL_CONTEXT_PROMPT = \"\"\"The following additional context is provided from other functions.\\\n",
    "    Use it to substitute into any ${{#}} variables or other words in the problem.\\\n",
    "    \\n\\n${context}\\n\\nNote that context variables are not defined in code yet.\\\n",
    "You must extract the relevant numbers and directly put them in code.\"\"\"\n",
    "\n",
    "\n",
    "class ExecuteCode(BaseModel):\n",
    "    \"\"\"The input to the numexpr.evaluate() function.\"\"\"\n",
    "\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"The reasoning behind the code expression, including how context is included, if applicable.\",\n",
    "    )\n",
    "\n",
    "    code: str = Field(\n",
    "        ...,\n",
    "        description=\"The simple code expression to execute by numexpr.evaluate().\",\n",
    "    )\n",
    "\n",
    "\n",
    "def _evaluate_expression(expression: str) -> str:\n",
    "    try:\n",
    "        local_dict = {\"pi\": math.pi, \"e\": math.e}\n",
    "        output = str(\n",
    "            numexpr.evaluate(\n",
    "                expression.strip(),\n",
    "                global_dict={},  # restrict access to globals\n",
    "                local_dict=local_dict,  # add common mathematical functions\n",
    "            )\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise ValueError(\n",
    "            f'Failed to evaluate \"{expression}\". Raised error: {repr(e)}.'\n",
    "            \" Please try again with a valid numerical expression\"\n",
    "        )\n",
    "\n",
    "    # Remove any leading and trailing brackets from the output\n",
    "    return re.sub(r\"^\\[|\\]$\", \"\", output)\n",
    "\n",
    "\n",
    "def get_math_tool(llm: ChatOpenAI):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", _SYSTEM_PROMPT),\n",
    "            (\"user\", \"{problem}\"),\n",
    "            MessagesPlaceholder(variable_name=\"context\", optional=True),\n",
    "        ]\n",
    "    )\n",
    "    extractor = prompt | llm.with_structured_output(ExecuteCode)\n",
    "\n",
    "    def calculate_expression(\n",
    "        problem: str,\n",
    "        context: Optional[List[str]] = None,\n",
    "        config: Optional[RunnableConfig] = None,\n",
    "    ):\n",
    "        chain_input = {\"problem\": problem}\n",
    "        if context:\n",
    "            context_str = \"\\n\".join(context)\n",
    "            if context_str.strip():\n",
    "                context_str = _ADDITIONAL_CONTEXT_PROMPT.format(\n",
    "                    context=context_str.strip()\n",
    "                )\n",
    "                chain_input[\"context\"] = [SystemMessage(content=context_str)]\n",
    "        code_model = extractor.invoke(chain_input, config)\n",
    "        try:\n",
    "            return _evaluate_expression(code_model.code)\n",
    "        except Exception as e:\n",
    "            return repr(e)\n",
    "\n",
    "    return StructuredTool.from_function(\n",
    "        name=\"math\",\n",
    "        func=calculate_expression,\n",
    "        description=_MATH_DESCRIPTION,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4449d340",
   "metadata": {},
   "source": [
    "#### Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f06b74b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from typing import (\n",
    "    Any,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.output_parsers.transform import BaseTransformOutputParser\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import BaseTool\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "THOUGHT_PATTERN = r\"Thought: ([^\\n]*)\"\n",
    "ACTION_PATTERN = r\"\\n*(\\d+)\\. (\\w+)\\((.*)\\)(\\s*#\\w+\\n)?\"\n",
    "# $1 or ${1} -> 1\n",
    "ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n",
    "END_OF_PLAN = \"\"\n",
    "\n",
    "\n",
    "### Helper functions\n",
    "\n",
    "\n",
    "def _ast_parse(arg: str) -> Any:\n",
    "    try:\n",
    "        return ast.literal_eval(arg)\n",
    "    except:  # noqa\n",
    "        return arg\n",
    "\n",
    "\n",
    "def _parse_llm_compiler_action_args(args: str, tool: Union[str, BaseTool]) -> list[Any]:\n",
    "    \"\"\"Parse arguments from a string.\"\"\"\n",
    "    if args == \"\":\n",
    "        return ()\n",
    "    if isinstance(tool, str):\n",
    "        return ()\n",
    "    extracted_args = {}\n",
    "    tool_key = None\n",
    "    prev_idx = None\n",
    "    for key in tool.args.keys():\n",
    "        # Split if present\n",
    "        if f\"{key}=\" in args:\n",
    "            idx = args.index(f\"{key}=\")\n",
    "            if prev_idx is not None:\n",
    "                extracted_args[tool_key] = _ast_parse(\n",
    "                    args[prev_idx:idx].strip().rstrip(\",\")\n",
    "                )\n",
    "            args = args.split(f\"{key}=\", 1)[1]\n",
    "            tool_key = key\n",
    "            prev_idx = 0\n",
    "    if prev_idx is not None:\n",
    "        extracted_args[tool_key] = _ast_parse(\n",
    "            args[prev_idx:].strip().rstrip(\",\").rstrip(\")\")\n",
    "        )\n",
    "    return extracted_args\n",
    "\n",
    "\n",
    "def default_dependency_rule(idx, args: str):\n",
    "    matches = re.findall(ID_PATTERN, args)\n",
    "    numbers = [int(match) for match in matches]\n",
    "    return idx in numbers\n",
    "\n",
    "\n",
    "def _get_dependencies_from_graph(\n",
    "    idx: int, tool_name: str, args: Dict[str, Any]\n",
    ") -> dict[str, list[str]]:\n",
    "    \"\"\"Get dependencies from a graph.\"\"\"\n",
    "    if tool_name == \"join\":\n",
    "        return list(range(1, idx))\n",
    "    return [i for i in range(1, idx) if default_dependency_rule(i, str(args))]\n",
    "\n",
    "\n",
    "class Task(TypedDict):\n",
    "    idx: int\n",
    "    tool: BaseTool\n",
    "    args: list\n",
    "    dependencies: Dict[str, list]\n",
    "    thought: Optional[str]\n",
    "\n",
    "\n",
    "def instantiate_task(\n",
    "    tools: Sequence[BaseTool],\n",
    "    idx: int,\n",
    "    tool_name: str,\n",
    "    args: Union[str, Any],\n",
    "    thought: Optional[str] = None,\n",
    ") -> Task:\n",
    "    if tool_name == \"join\":\n",
    "        tool = \"join\"\n",
    "    else:\n",
    "        try:\n",
    "            tool = tools[[tool.name for tool in tools].index(tool_name)]\n",
    "        except ValueError as e:\n",
    "            raise OutputParserException(f\"Tool {tool_name} not found.\") from e\n",
    "    tool_args = _parse_llm_compiler_action_args(args, tool)\n",
    "    dependencies = _get_dependencies_from_graph(idx, tool_name, tool_args)\n",
    "\n",
    "    return Task(\n",
    "        idx=idx,\n",
    "        tool=tool,\n",
    "        args=tool_args,\n",
    "        dependencies=dependencies,\n",
    "        thought=thought,\n",
    "    )\n",
    "\n",
    "\n",
    "class LLMCompilerPlanParser(BaseTransformOutputParser[dict], extra=\"allow\"):\n",
    "    \"\"\"Planning output parser.\"\"\"\n",
    "\n",
    "    tools: List[BaseTool]\n",
    "\n",
    "    def _transform(self, input: Iterator[Union[str, BaseMessage]]) -> Iterator[Task]:\n",
    "        texts = []\n",
    "        # TODO: Cleanup tuple state tracking here.\n",
    "        thought = None\n",
    "        for chunk in input:\n",
    "            # Assume input is str. TODO: support vision/other formats\n",
    "            text = chunk if isinstance(chunk, str) else str(chunk.content)\n",
    "            for task, thought in self.ingest_token(text, texts, thought):\n",
    "                yield task\n",
    "        # Final possible task\n",
    "        if texts:\n",
    "            task, _ = self._parse_task(\"\".join(texts), thought)\n",
    "            if task:\n",
    "                yield task\n",
    "\n",
    "    def parse(self, text: str) -> List[Task]:\n",
    "        return list(self._transform([text]))\n",
    "\n",
    "    def stream(\n",
    "        self,\n",
    "        input: str | BaseMessage,\n",
    "        config: RunnableConfig | None = None,\n",
    "        **kwargs: Any | None,\n",
    "    ) -> Iterator[Task]:\n",
    "        yield from self.transform([input], config, **kwargs)\n",
    "\n",
    "    def ingest_token(\n",
    "        self, token: str, buffer: List[str], thought: Optional[str]\n",
    "    ) -> Iterator[Tuple[Optional[Task], str]]:\n",
    "        buffer.append(token)\n",
    "        if \"\\n\" in token:\n",
    "            buffer_ = \"\".join(buffer).split(\"\\n\")\n",
    "            suffix = buffer_[-1]\n",
    "            for line in buffer_[:-1]:\n",
    "                task, thought = self._parse_task(line, thought)\n",
    "                if task:\n",
    "                    yield task, thought\n",
    "            buffer.clear()\n",
    "            buffer.append(suffix)\n",
    "\n",
    "    def _parse_task(self, line: str, thought: Optional[str] = None):\n",
    "        task = None\n",
    "        if match := re.match(THOUGHT_PATTERN, line):\n",
    "            # Optionally, action can be preceded by a thought\n",
    "            thought = match.group(1)\n",
    "        elif match := re.match(ACTION_PATTERN, line):\n",
    "            # if action is parsed, return the task, and clear the buffer\n",
    "            idx, tool_name, args, _ = match.groups()\n",
    "            idx = int(idx)\n",
    "            task = instantiate_task(\n",
    "                tools=self.tools,\n",
    "                idx=idx,\n",
    "                tool_name=tool_name,\n",
    "                args=args,\n",
    "                thought=thought,\n",
    "            )\n",
    "            thought = None\n",
    "        # Else it is just dropped\n",
    "        return task, thought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0173568f",
   "metadata": {},
   "source": [
    "#### Define Tools\n",
    "- 먼저 데모에서 에이전트가 사용할 도구를 정의. 검색 엔진 + 계산기 콤보 클래스를 제공."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d0459f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate = get_math_tool(ChatOpenAI(model=\"gpt-4o-2024-11-20\"))\n",
    "search = TavilySearchResults(\n",
    "    max_results=1,\n",
    "    description='tavily_search_results_json(query=\"the search query\") - a search engine.',\n",
    ")\n",
    "\n",
    "tools = [search, calculate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fd61d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'37'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate.invoke(\n",
    "    {\n",
    "        \"problem\": \"What's the temp of sf + 5?\",\n",
    "        \"context\": [\"Thet empreature of sf is 32 degrees\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48e4818",
   "metadata": {},
   "source": [
    "#### Planner\n",
    "원래 소스 코드 에서 크게 가져온 이 플래너는 입력 질문을 받고 실행할 작업 목록을 생성.\n",
    "\n",
    "이전 계획이 제공되면 계획을 다시 세우라는 지시가 나오는데, 이는 첫 번째 작업 배치를 완료한 후에 에이전트가 더 많은 조치를 취해야 하는 경우 유용.\n",
    "\n",
    "아래 코드는 플래너의 프롬프트 템플릿을 구성하고 LLM 및 출력 파서(에 정의됨)로 구성. 출력 파서는 다음 형식으로 작업 목록을 처리.\n",
    "\n",
    "```\n",
    "1. tool_1(arg1=\"arg1\", arg2=3.5, ...)\n",
    "Thought: I then want to find out Y by using tool_2\n",
    "2. tool_2(arg1=\"\", arg2=\"${1}\")'\n",
    "3. join()<END_OF_PLAN>\"\n",
    "```\n",
    "\n",
    "\"Thought\" 라인은 선택 사항. ${#}플레이스홀더는 변수. 이는 도구(작업) 출력을 다른 도구로 라우팅하는 데 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0d282db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# prompt = hub.pull(\"wfh/llm-compiler\")\n",
    "# print(prompt.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c35d7268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following \u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m types:\n",
      "\u001b[33;1m\u001b[1;3m{tool_descriptions}\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m. join(): Collects and combines results from prior actions.\n",
      "\n",
      " - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n",
      " - join should always be the last action in the plan, and will be called in two scenarios:\n",
      "   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n",
      "   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\n",
      " - Each action described above contains input/output types and description.\n",
      "    - You must strictly adhere to the input and output types for each action.\n",
      "    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n",
      " - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n",
      " - Each action MUST have a unique ID, which is strictly increasing.\n",
      " - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n",
      " - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\n",
      " - Ensure the plan maximizes parallelizability.\n",
      " - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n",
      " - Never introduce new actions other than the ones provided.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{messages}\u001b[0m\n",
      "\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Remember, ONLY respond with the task list in the correct format! E.g.:\n",
      "idx. tool(arg_name=args)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following {num_tools} types:\n",
    "{tool_descriptions}\n",
    "{num_tools}. join(): Collects and combines results from prior actions.\n",
    "\n",
    " - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n",
    " - join should always be the last action in the plan, and will be called in two scenarios:\n",
    "   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n",
    "   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\n",
    " - Each action described above contains input/output types and description.\n",
    "    - You must strictly adhere to the input and output types for each action.\n",
    "    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n",
    " - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n",
    " - Each action MUST have a unique ID, which is strictly increasing.\n",
    " - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n",
    " - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\n",
    " - Ensure the plan maximizes parallelizability.\n",
    " - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n",
    " - Never introduce new actions other than the ones provided.\"\"\"),\n",
    "        MessagesPlaceholder(\"messages\"),\n",
    "        (\"system\", \"\"\"Remember, ONLY respond with the task list in the correct format! E.g.:\n",
    "idx. tool(arg_name=args)\"\"\")\n",
    "    ]\n",
    ")\n",
    "print(prompt.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e41e1b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_planner(\n",
    "    llm: BaseChatModel, tools: Sequence[BaseTool], base_prompt: ChatPromptTemplate\n",
    "):\n",
    "    tool_descriptions = \"\\n\".join(\n",
    "        f\"{i+1}. {tool.description}\\n\"\n",
    "        for i, tool in enumerate(\n",
    "            tools\n",
    "        )  # +1 to offset the 0 starting index, we want it count normally from 1.\n",
    "    )\n",
    "    planner_prompt = base_prompt.partial(\n",
    "        replan=\"\",\n",
    "        num_tools=len(tools)\n",
    "        + 1,  # Add one because we're adding the join() tool at the end.\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "    replanner_prompt = base_prompt.partial(\n",
    "        replan=' - You are given \"Previous Plan\" which is the plan that the previous agent created along with the execution results '\n",
    "        \"(given as Observation) of each plan and a general thought (given as Thought) about the executed results.\"\n",
    "        'You MUST use these information to create the next plan under \"Current Plan\".\\n'\n",
    "        ' - When starting the Current Plan, you should start with \"Thought\" that outlines the strategy for the next plan.\\n'\n",
    "        \" - In the Current Plan, you should NEVER repeat the actions that are already executed in the Previous Plan.\\n\"\n",
    "        \" - You must continue the task index from the end of the previous one. Do not repeat task indices.\",\n",
    "        num_tools=len(tools) + 1,\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "\n",
    "    def should_replan(state: list):\n",
    "        # Context is passed as a system message\n",
    "        return isinstance(state[-1], SystemMessage)\n",
    "\n",
    "    def wrap_messages(state: list):\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    def wrap_and_get_last_index(state: list):\n",
    "        next_task = 0\n",
    "        for message in state[::-1]:\n",
    "            if isinstance(message, FunctionMessage):\n",
    "                next_task = message.additional_kwargs[\"idx\"] + 1\n",
    "                break\n",
    "        state[-1].content = state[-1].content + f\" - Begin counting at : {next_task}\"\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    return (\n",
    "        RunnableBranch(\n",
    "            (should_replan, wrap_and_get_last_index | replanner_prompt),\n",
    "            wrap_messages | planner_prompt,\n",
    "        )\n",
    "        | llm\n",
    "        | LLMCompilerPlanParser(tools=tools)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f40b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-2024-11-20\")\n",
    "planner = create_planner(llm, tools, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94015138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description='tavily_search_results_json(query=\"the search query\") - a search engine.' max_results=1 api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********')) {'query': 'current temperature in San Francisco'}\n",
      "---\n",
      "name='math' description='math(problem: str, context: Optional[list[str]]) -> float:\\n - Solves the provided math problem.\\n - `problem` can be either a simple math problem (e.g. \"1 + 3\") or a word problem (e.g. \"how many apples are there if there are 3 apples and 2 apples\").\\n - You cannot calculate multiple expressions in one call. For instance, `math(\\'1 + 3, 2 + 4\\')` does not work. If you need to calculate multiple expressions, you need to call them separately like `math(\\'1 + 3\\')` and then `math(\\'2 + 4\\')`\\n - Minimize the number of `math` actions as much as possible. For instance, instead of calling 2. math(\"what is the 10% of $1\") and then call 3. math(\"$1 + $2\"), you MUST call 2. math(\"what is the 110% of $1\") instead, which will reduce the number of math actions.\\n - You can optionally provide a list of strings as `context` to help the agent solve the problem. If there are multiple contexts you need to answer the question, you can provide them as a list of strings.\\n - `math` action will not see the output of the previous actions unless you provide it as `context`. You MUST provide the output of the previous actions as `context` if you need to do math on it.\\n - You MUST NEVER provide `search` type action\\'s outputs as a variable in the `problem` argument. This is because `search` returns a text blob that contains the information about the entity, not a number or value. Therefore, when you need to provide an output of `search` action, you MUST provide it as a `context` argument to `math` action. For example, 1. search(\"Barack Obama\") and then 2. math(\"age of $1\") is NEVER allowed. Use 2. math(\"age of Barack Obama\", context=[\"$1\"]) instead.\\n - When you ask a question about `context`, specify the units. For instance, \"what is xx in height?\" or \"what is xx in millions?\" instead of \"what is xx?\"' args_schema=<class 'langchain_core.utils.pydantic.math'> func=<function get_math_tool.<locals>.calculate_expression at 0x7effe57eaf20> {'problem': 'what is the cube of the temperature in San Francisco?', 'context': ['$1']}\n",
      "---\n",
      "join ()\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "example_question = \"What's the temperature in SF raised to the 3rd power?\"\n",
    "\n",
    "for task in planner.stream([HumanMessage(content=example_question)]):\n",
    "    print(task[\"tool\"], task[\"args\"])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641e07c0",
   "metadata": {},
   "source": [
    "#### Task Fetching Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e87ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from typing import Any, Dict, Iterable, List, Union\n",
    "\n",
    "from langchain_core.runnables import (\n",
    "    chain as as_runnable,\n",
    ")\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "def _get_observations(messages: List[BaseMessage]) -> Dict[int, Any]:\n",
    "    # Get all previous tool responses\n",
    "    results = {}\n",
    "    for message in messages[::-1]:\n",
    "        if isinstance(message, FunctionMessage):\n",
    "            results[int(message.additional_kwargs[\"idx\"])] = message.content\n",
    "    return results\n",
    "\n",
    "\n",
    "class SchedulerInput(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    tasks: Iterable[Task]\n",
    "\n",
    "\n",
    "def _execute_task(task, observations, config):\n",
    "    tool_to_use = task[\"tool\"]\n",
    "    if isinstance(tool_to_use, str):\n",
    "        return tool_to_use\n",
    "    args = task[\"args\"]\n",
    "    try:\n",
    "        if isinstance(args, str):\n",
    "            resolved_args = _resolve_arg(args, observations)\n",
    "        elif isinstance(args, dict):\n",
    "            resolved_args = {\n",
    "                key: _resolve_arg(val, observations) for key, val in args.items()\n",
    "            }\n",
    "        else:\n",
    "            # This will likely fail\n",
    "            resolved_args = args\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.)\"\n",
    "            f\" Args could not be resolved. Error: {repr(e)}\"\n",
    "        )\n",
    "    try:\n",
    "        return tool_to_use.invoke(resolved_args, config)\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.\"\n",
    "            + f\" Args resolved to {resolved_args}. Error: {repr(e)})\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _resolve_arg(arg: Union[str, Any], observations: Dict[int, Any]):\n",
    "    # $1 or ${1} -> 1\n",
    "    ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n",
    "\n",
    "    def replace_match(match):\n",
    "        # If the string is ${123}, match.group(0) is ${123}, and match.group(1) is 123.\n",
    "\n",
    "        # Return the match group, in this case the index, from the string. This is the index\n",
    "        # number we get back.\n",
    "        idx = int(match.group(1))\n",
    "        return str(observations.get(idx, match.group(0)))\n",
    "\n",
    "    # For dependencies on other tasks\n",
    "    if isinstance(arg, str):\n",
    "        return re.sub(ID_PATTERN, replace_match, arg)\n",
    "    elif isinstance(arg, list):\n",
    "        return [_resolve_arg(a, observations) for a in arg]\n",
    "    else:\n",
    "        return str(arg)\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_task(task_inputs, config):\n",
    "    task: Task = task_inputs[\"task\"]\n",
    "    observations: Dict[int, Any] = task_inputs[\"observations\"]\n",
    "    try:\n",
    "        observation = _execute_task(task, observations, config)\n",
    "    except Exception:\n",
    "        import traceback\n",
    "\n",
    "        observation = traceback.format_exception()  # repr(e) +\n",
    "    observations[task[\"idx\"]] = observation\n",
    "\n",
    "\n",
    "def schedule_pending_task(\n",
    "    task: Task, observations: Dict[int, Any], retry_after: float = 0.2\n",
    "):\n",
    "    while True:\n",
    "        deps = task[\"dependencies\"]\n",
    "        if deps and (any([dep not in observations for dep in deps])):\n",
    "            # Dependencies not yet satisfied\n",
    "            time.sleep(retry_after)\n",
    "            continue\n",
    "        schedule_task.invoke({\"task\": task, \"observations\": observations})\n",
    "        break\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_tasks(scheduler_input: SchedulerInput) -> List[FunctionMessage]:\n",
    "    \"\"\"Group the tasks into a DAG schedule.\"\"\"\n",
    "    # For streaming, we are making a few simplifying assumption:\n",
    "    # 1. The LLM does not create cyclic dependencies\n",
    "    # 2. That the LLM will not generate tasks with future deps\n",
    "    # If this ceases to be a good assumption, you can either\n",
    "    # adjust to do a proper topological sort (not-stream)\n",
    "    # or use a more complicated data structure\n",
    "    tasks = scheduler_input[\"tasks\"]\n",
    "    args_for_tasks = {}\n",
    "    messages = scheduler_input[\"messages\"]\n",
    "    # If we are re-planning, we may have calls that depend on previous\n",
    "    # plans. Start with those.\n",
    "    observations = _get_observations(messages)\n",
    "    task_names = {}\n",
    "    originals = set(observations)\n",
    "    # ^^ We assume each task inserts a different key above to\n",
    "    # avoid race conditions...\n",
    "    futures = []\n",
    "    retry_after = 0.25  # Retry every quarter second\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for task in tasks:\n",
    "            deps = task[\"dependencies\"]\n",
    "            task_names[task[\"idx\"]] = (\n",
    "                task[\"tool\"] if isinstance(task[\"tool\"], str) else task[\"tool\"].name\n",
    "            )\n",
    "            args_for_tasks[task[\"idx\"]] = task[\"args\"]\n",
    "            if (\n",
    "                # Depends on other tasks\n",
    "                deps and (any([dep not in observations for dep in deps]))\n",
    "            ):\n",
    "                futures.append(\n",
    "                    executor.submit(\n",
    "                        schedule_pending_task, task, observations, retry_after\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # No deps or all deps satisfied\n",
    "                # can schedule now\n",
    "                schedule_task.invoke(dict(task=task, observations=observations))\n",
    "                # futures.append(executor.submit(schedule_task.invoke, dict(task=task, observations=observations)))\n",
    "\n",
    "        # All tasks have been submitted or enqueued\n",
    "        # Wait for them to complete\n",
    "        wait(futures)\n",
    "    # Convert observations to new tool messages to add to the state\n",
    "    new_observations = {\n",
    "        k: (task_names[k], args_for_tasks[k], observations[k])\n",
    "        for k in sorted(observations.keys() - originals)\n",
    "    }\n",
    "    tool_messages = [\n",
    "        FunctionMessage(\n",
    "            name=name,\n",
    "            content=str(obs),\n",
    "            additional_kwargs={\"idx\": k, \"args\": task_args},\n",
    "            tool_call_id=k,\n",
    "        )\n",
    "        for k, (name, task_args, obs) in new_observations.items()\n",
    "    ]\n",
    "    return tool_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a47a990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def plan_and_schedule(state):\n",
    "    messages = state[\"messages\"]\n",
    "    tasks = planner.stream(messages)\n",
    "    # Begin executing the planner immediately\n",
    "    try:\n",
    "        tasks = itertools.chain([next(tasks)], tasks)\n",
    "    except StopIteration:\n",
    "        # Handle the case where tasks is empty.\n",
    "        tasks = iter([])\n",
    "    scheduled_tasks = schedule_tasks.invoke(\n",
    "        {\n",
    "            \"messages\": messages,\n",
    "            \"tasks\": tasks,\n",
    "        }\n",
    "    )\n",
    "    return {\"messages\": scheduled_tasks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52347928",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_messages = plan_and_schedule.invoke(\n",
    "    {\"messages\": [HumanMessage(content=example_question)]}\n",
    ")[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfb47ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FunctionMessage(content=\"[{'title': 'Current Weather - San Francisco, CA - AccuWeather', 'url': 'https://www.accuweather.com/en/us/san-francisco/94103/current-weather/347629', 'content': 'Current Weather\\\\n9:14 PM\\\\n48°F\\\\nClear\\\\nRealFeel® 49°\\\\nChilly\\\\nRealFeel Guide\\\\nChilly\\\\n40° to 52°\\\\nJacket or sweater is recommended.\\\\nLEARN MORE\\\\nRealFeel®\\\\n49°\\\\nWind\\\\nWNW 3 mph\\\\nWind Gusts\\\\n5 mph\\\\nHumidity\\\\n83%\\\\nIndoor Humidity\\\\n40% (Ideal Humidity)\\\\nDew Point\\\\n43° F\\\\nPressure\\\\n↓ 30.23 in\\\\nCloud Cover\\\\n2%\\\\nVisibility\\\\n9 mi\\\\nCloud Ceiling\\\\n33400 ft\\\\nNight\\\\n2/7\\\\n41°Lo\\\\nRealFeel® 37°\\\\nCold\\\\nRealFeel Guide\\\\nCold\\\\n25° to 39°\\\\nCoats and hats are appropriate, consider gloves and a scarf.\\\\nLEARN MORE\\\\nClear to partly cloudy and chilly [...] Rise 7:08 AM\\\\nSet 5:40 PM\\\\n Waxing Gibbous\\\\nRise 12:42 PM\\\\nSet 4:37 AM\\\\nTemperature History\\\\n2/7\\\\nHigh\\\\nLow\\\\nForecast\\\\n56°\\\\n41°\\\\nAverage\\\\n60°\\\\n48°\\\\nLast Year\\\\n53°\\\\n48°\\\\nRecord\\\\n73°\\\\n2006\\\\n34°\\\\n1989\\\\nFurther Ahead', 'score': 0.76427686}]\", additional_kwargs={'idx': 1, 'args': {'query': 'current temperature in San Francisco'}}, response_metadata={}, name='tavily_search_results_json', tool_call_id=1),\n",
       " FunctionMessage(content='110592', additional_kwargs={'idx': 2, 'args': {'problem': 'current temperature in San Francisco raised to the 3rd power', 'context': ['$1']}}, response_metadata={}, name='math', tool_call_id=2),\n",
       " FunctionMessage(content='join', additional_kwargs={'idx': 3, 'args': ()}, response_metadata={}, name='join', tool_call_id=3)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418f4c6b",
   "metadata": {},
   "source": [
    "#### joiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e74c896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Solve a question answering task. Here are some guidelines:\n",
      " - In the Assistant Scratchpad, you will be given results of a plan you have executed to answer the user's question.\n",
      " - Thought needs to reason about the question based on the Observations in 1-2 sentences.\n",
      " - Ignore irrelevant action results.\n",
      " - If the required information is present, give a concise but complete and helpful answer to the user's question.\n",
      " - If you are unable to give a satisfactory finishing answer, replan to get the required information. Respond in the following format:\n",
      "\n",
      "Thought: <reason about the task results and whether you have sufficient information to answer the question>\n",
      "Action: <action to take>\n",
      "Available actions:\n",
      " (1) Finish(the final answer to return to the user): returns the answer and finishes the task.\n",
      " (2) Replan(the reasoning and other information that will help you plan again. Can be a line of any length): instructs why we must replan\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{messages}\u001b[0m\n",
      "\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Using the above previous actions, decide whether to replan or finish. If all the required information is present. You may finish. If you have made many attempts to find the information without success, admit so and respond with whatever information you have gathered so the user can work well with you.\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{examples}\u001b[0m\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    \"\"\"The final response/answer.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Replan(BaseModel):\n",
    "    feedback: str = Field(\n",
    "        description=\"Analysis of the previous attempts and recommendations on what needs to be fixed.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class JoinOutputs(BaseModel):\n",
    "    \"\"\"Decide whether to replan or whether you can return the final response.\"\"\"\n",
    "\n",
    "    thought: str = Field(\n",
    "        description=\"The chain of thought reasoning for the selected action\"\n",
    "    )\n",
    "    action: Union[FinalResponse, Replan]\n",
    "\n",
    "joiner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"Solve a question answering task. Here are some guidelines:\n",
    " - In the Assistant Scratchpad, you will be given results of a plan you have executed to answer the user's question.\n",
    " - Thought needs to reason about the question based on the Observations in 1-2 sentences.\n",
    " - Ignore irrelevant action results.\n",
    " - If the required information is present, give a concise but complete and helpful answer to the user's question.\n",
    " - If you are unable to give a satisfactory finishing answer, replan to get the required information. Respond in the following format:\n",
    "\n",
    "Thought: <reason about the task results and whether you have sufficient information to answer the question>\n",
    "Action: <action to take>\n",
    "Available actions:\n",
    " (1) Finish(the final answer to return to the user): returns the answer and finishes the task.\n",
    " (2) Replan(the reasoning and other information that will help you plan again. Can be a line of any length): instructs why we must replan\"\"\"),\n",
    "        MessagesPlaceholder(\"messages\"),\n",
    "        (\"system\", \"\"\"Using the above previous actions, decide whether to replan or finish. If all the required information is present. You may finish. If you have made many attempts to find the information without success, admit so and respond with whatever information you have gathered so the user can work well with you.\n",
    "\n",
    "{examples}\"\"\")\n",
    "    ]\n",
    ")\n",
    "joiner_prompt = joiner_prompt.partial(examples=\"\")\n",
    "\n",
    "print(joiner_prompt.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d3ad5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = joiner_prompt | llm.with_structured_output(\n",
    "    JoinOutputs, method=\"function_calling\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b7bd273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_joiner_output(decision: JoinOutputs) -> List[BaseMessage]:\n",
    "    response = [AIMessage(content=f\"Thought: {decision.thought}\")]\n",
    "    if isinstance(decision.action, Replan):\n",
    "        return {\n",
    "            \"messages\": response\n",
    "            + [\n",
    "                SystemMessage(\n",
    "                    content=f\"Context from last attempt: {decision.action.feedback}\"\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        return {\"messages\": response + [AIMessage(content=decision.action.response)]}\n",
    "\n",
    "\n",
    "def select_recent_messages(state) -> dict:\n",
    "    messages = state[\"messages\"]\n",
    "    selected = []\n",
    "    for msg in messages[::-1]:\n",
    "        selected.append(msg)\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            break\n",
    "    return {\"messages\": selected[::-1]}\n",
    "\n",
    "\n",
    "joiner = select_recent_messages | runnable | _parse_joiner_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d771b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=example_question)] + tool_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61831f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content='Thought: I found the current temperature in San Francisco as 48°F from the search results. After performing the mathematical operation to raise this temperature to the 3rd power, the result is 110592.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The current temperature in San Francisco is 48°F, and when raised to the 3rd power, it equals 110,592.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joiner.invoke({\"messages\": input_messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ab6d9b",
   "metadata": {},
   "source": [
    "#### langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "885cee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 1.  Define vertices\n",
    "# We defined plan_and_schedule above already\n",
    "# Assign each node to a state variable to update\n",
    "graph_builder.add_node(\"plan_and_schedule\", plan_and_schedule)\n",
    "graph_builder.add_node(\"join\", joiner)\n",
    "\n",
    "\n",
    "## Define edges\n",
    "graph_builder.add_edge(\"plan_and_schedule\", \"join\")\n",
    "\n",
    "### This condition determines looping logic\n",
    "\n",
    "\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if isinstance(messages[-1], AIMessage):\n",
    "        return END\n",
    "    return \"plan_and_schedule\"\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"join\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "graph_builder.add_edge(START, \"plan_and_schedule\")\n",
    "chain = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9371d633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content=\"[{'title': 'Economy of New York (state) - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Economy_of_New_York_(state)', 'content': 'Jump to content\\\\nMain menu\\\\nSearch\\\\nDonate\\\\nCreate account\\\\nLog in\\\\nPersonal tools\\\\nToggle the table of contents\\\\nEconomy of New York (state)\\\\n1 language\\\\nArticle\\\\nTalk\\\\nRead\\\\nEdit\\\\nView history\\\\nTools\\\\nFrom Wikipedia, the free encyclopedia\\\\nThis article is about the overall economy of New York State. For the economy of New York City, see Economy of New York City.\\\\nEconomy of New York\\\\nNew York City, the economic capital of New York (state)\\\\nStatistics\\\\nGDP $2.3 trillion (2024)[1]\\\\nGDP per capita  $117,332 (2024)[2] [...] The economy of the State of New York is reflected in its gross state product in 2024 of $2.284 trillion, ranking third in size behind the larger states of California and Texas. If New York State were an independent nation, it would rank as the 10th largest economy in the world by nominal GDP. However, in 2019, the multi-state, New York City-centered metropolitan statistical area produced a gross metropolitan product (GMP) of $US2.0 trillion, ranking first nationally by a wide margin and would', 'score': 0.92620385}]\", additional_kwargs={'idx': 1, 'args': {'query': 'GDP of New York'}}, response_metadata={}, name='tavily_search_results_json', id='5220579a-7a60-4fe5-888c-0fec6258af3b', tool_call_id=1), FunctionMessage(content='join', additional_kwargs={'idx': 2, 'args': ()}, response_metadata={}, name='join', id='e4036853-4435-4935-8504-002942c24982', tool_call_id=2)]}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content='Thought: The GDP of New York State for 2024 is clearly stated in the search result.', additional_kwargs={}, response_metadata={}, id='d6467322-75d1-4faa-bad7-4eaed08442d3'), AIMessage(content='The GDP of New York State for 2024 is approximately $2.3 trillion.', additional_kwargs={}, response_metadata={}, id='58d4a29c-357b-4b91-9cea-b930a45d615d')]}}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for step in chain.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"What's the GDP of New York?\")]}\n",
    "):\n",
    "    print(step)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5e01c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GDP of New York State for 2024 is approximately $2.3 trillion.\n"
     ]
    }
   ],
   "source": [
    "print(step[\"join\"][\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd8d645d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content='[{\\'title\\': \\'Cookie (cockatoo) - Wikipedia\\', \\'url\\': \\'https://en.wikipedia.org/wiki/Cookie_(cockatoo)\\', \\'content\\': \"Cookie (June 30, 1933 – August 27, 2016) was a male pink cockatoo (also known as Major Mitchell\\'s cockatoo) residing at Brookfield Zoo, near Chicago, Illinois, United States. He was believed to be the oldest member of his species alive in captivity, at the age of 82 in June 2015,[1][2] having significantly exceeded the average lifespan for his kind.[3] He was one of the longest-lived birds on record[4] and was recognised by the Guinness World Records as the oldest living parrot in the world.[5]\", \\'score\\': 0.9132521}]', additional_kwargs={'idx': 1, 'args': {'query': 'oldest parrot alive'}}, response_metadata={}, name='tavily_search_results_json', id='4edf22c5-d6d6-47c6-aafb-87dcf9f3166a', tool_call_id=1), FunctionMessage(content='[{\\'title\\': \\'Parrot Lifespan: How Long Do Pet Parrots Live? | Turlock Vets\\', \\'url\\': \\'https://www.turlockvet.com/site/blog/2023/07/15/parrot-lifespan--how-long-pet-parrots-live\\', \\'content\\': \"The largest impact on a parrot\\'s lifespan will be the type of parrot it is. Generally, the average lifespan of smaller species of parrots such as Budgies and Cockatiels is about 5 - 15 years, while larger parrots such as African Greys, Conures, Macaws and Cockatoos can live for anywhere between 20 - 80 years or even more.\\\\nThis means that you may even need to consider what will happen to your parrot once you are gone since they generally tend to outlive their owners.\", \\'score\\': 0.9284155}]', additional_kwargs={'idx': 2, 'args': {'query': 'average lifespan of a parrot'}}, response_metadata={}, name='tavily_search_results_json', id='48e4f049-a70a-470f-9cb3-211ebb2292af', tool_call_id=2), FunctionMessage(content='32', additional_kwargs={'idx': 3, 'args': {'problem': 'difference between the age of the oldest parrot alive and the average lifespan of a parrot', 'context': ['$1', '$2']}}, response_metadata={}, name='math', id='4a5abbfe-c097-4231-943d-05e0b22376c4', tool_call_id=3), FunctionMessage(content='join', additional_kwargs={'idx': 4, 'args': ()}, response_metadata={}, name='join', id='eb8f8ddc-b1f3-418a-99be-5e6c617731eb', tool_call_id=4)]}}\n",
      "---\n",
      "{'join': {'messages': [AIMessage(content=\"Thought: I found that Cookie, a cockatoo, was believed to be the oldest living parrot, living to 82 years old, while the average lifespan of larger parrots ranges from 20 to 80 years. Comparing his lifespan to the average, it's evident that Cookie exceeded even the upper limit of the typical lifespan range. I now have enough information to answer the user's question.\", additional_kwargs={}, response_metadata={}, id='31aad54c-4f79-4158-984f-75221e13cfcf'), AIMessage(content='Cookie, a male pink cockatoo, was the oldest known parrot, living to 82 years old. Comparing this to the typical lifespan of larger parrots (20 to 80 years), Cookie exceeded even the upper limit of the average by 2 years.', additional_kwargs={}, response_metadata={}, id='44762a43-ccd7-41d8-b348-11bff4283c27')]}}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "steps = chain.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"What's the oldest parrot alive, and how much longer is that than the average?\"\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"recursion_limit\": 100,\n",
    "    },\n",
    ")\n",
    "for step in steps:\n",
    "    print(step)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cca7863f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie, a male pink cockatoo, was the oldest known parrot, living to 82 years old. Comparing this to the typical lifespan of larger parrots (20 to 80 years), Cookie exceeded even the upper limit of the average by 2 years.\n"
     ]
    }
   ],
   "source": [
    "print(step[\"join\"][\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf3d5d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content='3307.0', additional_kwargs={'idx': 1, 'args': {'problem': '3*(4+5)/0.5 + 3245 + 8'}}, response_metadata={}, name='math', id='dd8f13a8-972d-47b8-bcbd-04262c1f7321', tool_call_id=1), FunctionMessage(content='7.565011820330969', additional_kwargs={'idx': 2, 'args': {'problem': '32/4.23'}}, response_metadata={}, name='math', id='887a778b-6f84-4d1b-8af6-1a73dc7040be', tool_call_id=2), FunctionMessage(content='3314.565011820331', additional_kwargs={'idx': 3, 'args': {'problem': '$1 + $2', 'context': ['$1', '$2']}}, response_metadata={}, name='math', id='73dc2a8f-18ef-4c9e-9ddb-9c8cf49ed639', tool_call_id=3), FunctionMessage(content='join', additional_kwargs={'idx': 4, 'args': ()}, response_metadata={}, name='join', id='cb1499f5-6fc4-401b-8ba6-b541693e00b5', tool_call_id=4)]}}\n",
      "{'join': {'messages': [AIMessage(content='Thought: I now have all the required calculations: the first value to be 3307.0, the second value to be approximately 7.565 (accurate to several decimal points as 7.565011820330969), and their sum is 3314.565011820331.', additional_kwargs={}, response_metadata={}, id='931089b2-9b65-45bf-851b-ca10d6e81085'), AIMessage(content='The value of ((3*(4+5)/0.5)+3245) + 8 is 3307, the value of 32/4.23 is approximately 7.565, and their sum is approximately 3314.565.', additional_kwargs={}, response_metadata={}, id='7fb1996f-94ee-4c25-8215-c38e4ef236ef')]}}\n"
     ]
    }
   ],
   "source": [
    "for step in chain.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"What's ((3*(4+5)/0.5)+3245) + 8? What's 32/4.23? What's the sum of those two values?\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78293b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of ((3*(4+5)/0.5)+3245) + 8 is 3307, the value of 32/4.23 is approximately 7.565, and their sum is approximately 3314.565.\n"
     ]
    }
   ],
   "source": [
    "print(step[\"join\"][\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a316a1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': {'messages': [FunctionMessage(content='[{\\'title\\': \\'Weather for Tokyo, Japan - Time and Date\\', \\'url\\': \\'https://www.timeanddate.com/weather/japan/tokyo\\', \\'content\\': \\'Home \\\\xa0 Weather \\\\xa0 Japan \\\\xa0 Tokyo\\\\nWeather in Tokyo, Japan\\\\nTime/General\\\\nWeather\\\\nTime Zone\\\\nDST Changes\\\\nSun & Moon\\\\nWeather TodayWeather Hourly14 Day ForecastYesterday/Past WeatherClimate (Averages)\\\\nNow\\\\n32\\\\xa0°F\\\\nChilly.\\\\nFeels Like: 28\\\\xa0°F\\\\nForecast: 51 / 31\\\\xa0°F\\\\nWind: 3 mph ↑ from Northwest\\\\nLocation:   Tokyo\\\\nCurrent Time:   Feb 10, 2025 at 8:22:57 am\\\\nLatest Report:  Feb 10, 2025 at 7:00 am\\\\nVisibility: N/A\\\\nPressure:   30.07 \"Hg\\\\nHumidity:   64%\\\\nDew Point:  21\\\\xa0°F\\\\nUpcoming 5 hours [...] Now 9:00 am 10:00 am    11:00 am    12:00 pm    1:00 pm\\\\n32\\\\xa0°F   39\\\\xa0°F   43\\\\xa0°F   46\\\\xa0°F   48\\\\xa0°F   49\\\\xa0°F\\\\nSee more hour-by-hour weather\\\\nForecast for the next 48 hours\\\\n\\\\xa0   Monday  Tuesday\\\\n\\\\xa0   Morning Afternoon   Evening Night   Morning Afternoon   Evening\\\\nForecast                          \\\\nTemperature 39\\\\xa0°F   50\\\\xa0°F   41\\\\xa0°F   35\\\\xa0°F   41\\\\xa0°F   52\\\\xa0°F   40\\\\xa0°F\\\\n    Overcast.   Mostly cloudy.  Clear.  Clear.  Sunny.  Sunny.  Clear.\\\\nFeels Like  35\\\\xa0°F   49\\\\xa0°F   36\\\\xa0°F   28\\\\xa0°F   35\\\\xa0°F   48\\\\xa0°F   33\\\\xa0°F\\', \\'score\\': 0.8124067}]', additional_kwargs={'idx': 1, 'args': {'query': 'current temperature in Tokyo'}}, response_metadata={}, name='tavily_search_results_json', id='4953830e-3977-4ecf-8d41-30523167a59d', tool_call_id=1), FunctionMessage(content='join', additional_kwargs={'idx': 2, 'args': ()}, response_metadata={}, name='join', id='ea81f9d2-1967-41ee-b935-c536f86e416d', tool_call_id=2)]}}\n",
      "{'join': {'messages': [AIMessage(content=\"Thought: The current temperature in Tokyo is mentioned in the search result: 32°F, with a 'feels like' temperature of 28°F. This provides complete and sufficient data to create a flashcard summarizing the information.\", additional_kwargs={}, response_metadata={}, id='5fa20e22-de98-4516-9993-aece3721d471'), AIMessage(content='🌟 **Flashcard: Current Weather in Tokyo** 🌟\\n\\n🗾 **Location:** Tokyo, Japan\\n🌡️ **Current Temperature:** 32°F\\n🌬️ **Feels Like:** 28°F\\n📅 **Observation Time:** Feb 10, 2025, at 7:00 am\\n☁️ **Condition:** Chilly', additional_kwargs={}, response_metadata={}, id='b533c4fc-e4c5-4d27-8337-fcba886131e0')]}}\n"
     ]
    }
   ],
   "source": [
    "for step in chain.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Find the current temperature in Tokyo, then, respond with a flashcard summarizing this information\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3f8ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌟 **Flashcard: Current Weather in Tokyo** 🌟\n",
      "\n",
      "🗾 **Location:** Tokyo, Japan\n",
      "🌡️ **Current Temperature:** 32°F\n",
      "🌬️ **Feels Like:** 28°F\n",
      "📅 **Observation Time:** Feb 10, 2025, at 7:00 am\n",
      "☁️ **Condition:** Chilly\n"
     ]
    }
   ],
   "source": [
    "print(step[\"join\"][\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a3fad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
