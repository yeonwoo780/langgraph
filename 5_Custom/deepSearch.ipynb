{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac31d754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/code/langgraph'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "054146a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcadfce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db243eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get current date in a readable format\n",
    "def get_current_date():\n",
    "    return datetime.now().strftime(\"%B %d, %Y\")\n",
    "\n",
    "query_writer_instructions=\"\"\"Your goal is to generate a targeted web search query.\n",
    "\n",
    "<CONTEXT>\n",
    "Current date: {current_date}\n",
    "Please ensure your queries account for the most current information available as of this date.\n",
    "</CONTEXT>\n",
    "\n",
    "<TOPIC>\n",
    "{research_topic}\n",
    "</TOPIC>\n",
    "\n",
    "<FORMAT>\n",
    "Format your response as a JSON object with ALL three of these exact keys:\n",
    "   - \"query\": The actual search query string\n",
    "   - \"rationale\": Brief explanation of why this query is relevant\n",
    "</FORMAT>\n",
    "\n",
    "<EXAMPLE>\n",
    "Example output:\n",
    "{{\n",
    "    \"query\": \"machine learning transformer architecture explained\",\n",
    "    \"rationale\": \"Understanding the fundamental structure of transformer models\"\n",
    "}}\n",
    "</EXAMPLE>\n",
    "\n",
    "Provide your response in JSON format:\"\"\"\n",
    "\n",
    "summarizer_instructions=\"\"\"\n",
    "<GOAL>\n",
    "Generate a high-quality summary of the provided context.\n",
    "</GOAL>\n",
    "\n",
    "<REQUIREMENTS>\n",
    "When creating a NEW summary:\n",
    "1. Highlight the most relevant information related to the user topic from the search results\n",
    "2. Ensure a coherent flow of information\n",
    "\n",
    "When EXTENDING an existing summary:                                                                                                                 \n",
    "1. Read the existing summary and new search results carefully.                                                    \n",
    "2. Compare the new information with the existing summary.                                                         \n",
    "3. For each piece of new information:                                                                             \n",
    "    a. If it's related to existing points, integrate it into the relevant paragraph.                               \n",
    "    b. If it's entirely new but relevant, add a new paragraph with a smooth transition.                            \n",
    "    c. If it's not relevant to the user topic, skip it.                                                            \n",
    "4. Ensure all additions are relevant to the user's topic.                                                         \n",
    "5. Verify that your final output differs from the input summary.                                                                                                                                                            \n",
    "< /REQUIREMENTS >\n",
    "\n",
    "< FORMATTING >\n",
    "- Start directly with the updated summary, without preamble or titles. Do not use XML tags in the output.  \n",
    "< /FORMATTING >\n",
    "\n",
    "<Task>\n",
    "Think carefully about the provided Context first. Then generate a summary of the context to address the User Input.\n",
    "</Task>\n",
    "\"\"\"\n",
    "\n",
    "reflection_instructions = \"\"\"You are an expert research assistant analyzing a summary about {research_topic}.\n",
    "\n",
    "<GOAL>\n",
    "1. Identify knowledge gaps or areas that need deeper exploration\n",
    "2. Generate a follow-up question that would help expand your understanding\n",
    "3. Focus on technical details, implementation specifics, or emerging trends that weren't fully covered\n",
    "</GOAL>\n",
    "\n",
    "<REQUIREMENTS>\n",
    "Ensure the follow-up question is self-contained and includes necessary context for web search.\n",
    "</REQUIREMENTS>\n",
    "\n",
    "<FORMAT>\n",
    "Format your response as a JSON object with these exact keys:\n",
    "- knowledge_gap: Describe what information is missing or needs clarification\n",
    "- follow_up_query: Write a specific question to address this gap\n",
    "</FORMAT>\n",
    "\n",
    "<Task>\n",
    "Reflect carefully on the Summary to identify knowledge gaps and produce a follow-up query. Then, produce your output following this JSON format:\n",
    "{{\n",
    "    \"knowledge_gap\": \"The summary lacks information about performance metrics and benchmarks\",\n",
    "    \"follow_up_query\": \"What are typical performance benchmarks and metrics used to evaluate [specific technology]?\"\n",
    "}}\n",
    "</Task>\n",
    "\n",
    "Provide your analysis in JSON format:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "550472df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from typing import Any, List, Optional, Union, Dict\n",
    "\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    ")\n",
    "from langchain_core.outputs import ChatResult\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import Field\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ChatLMStudio(ChatOpenAI):\n",
    "    \"\"\"Chat model that uses LMStudio's OpenAI-compatible API.\"\"\"\n",
    "    \n",
    "    format: Optional[str] = Field(default=None, description=\"Format for the response (e.g., 'json')\")\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"gpt-4o-2024-08-06\",\n",
    "        temperature: float = 0.7,\n",
    "        format: Optional[str] = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        \"\"\"Initialize the ChatLMStudio.\n",
    "        \n",
    "        Args:\n",
    "            base_url: Base URL for LMStudio's OpenAI-compatible API\n",
    "            model: Model name to use\n",
    "            temperature: Temperature for sampling\n",
    "            format: Format for the response (e.g., \"json\")\n",
    "            api_key: API key (not actually used, but required by OpenAI client)\n",
    "            **kwargs: Additional arguments to pass to the OpenAI client\n",
    "        \"\"\"\n",
    "        # Initialize the base class\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.format = format\n",
    "        \n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \n",
    "        \"\"\"Generate a chat response using LMStudio's OpenAI-compatible API.\"\"\"\n",
    "        \n",
    "        if self.format == \"json\":\n",
    "            # Set response_format for JSON mode\n",
    "            kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "            logger.info(f\"Using response_format={kwargs['response_format']}\")\n",
    "        \n",
    "        # Call the parent class's _generate method\n",
    "        result = super()._generate(messages, stop, run_manager, **kwargs)\n",
    "        \n",
    "        # If JSON format is requested, try to clean up the response\n",
    "        if self.format == \"json\" and result.generations:\n",
    "            try:\n",
    "                # Get the raw text\n",
    "                raw_text = result.generations[0][0].text\n",
    "                logger.info(f\"Raw model response: {raw_text}\")\n",
    "                \n",
    "                # Try to find JSON in the response\n",
    "                json_start = raw_text.find('{')\n",
    "                json_end = raw_text.rfind('}') + 1\n",
    "                \n",
    "                if json_start >= 0 and json_end > json_start:\n",
    "                    # Extract just the JSON part\n",
    "                    json_text = raw_text[json_start:json_end]\n",
    "                    # Validate it's proper JSON\n",
    "                    json.loads(json_text)\n",
    "                    logger.info(f\"Cleaned JSON: {json_text}\")\n",
    "                    # Update the generation with the cleaned JSON\n",
    "                    result.generations[0][0].text = json_text\n",
    "                else:\n",
    "                    logger.warning(\"Could not find JSON in response\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing JSON response: {str(e)}\")\n",
    "                # If any error occurs during cleanup, just use the original response\n",
    "                pass\n",
    "                \n",
    "        return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "964764d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatLMStudio(format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b435ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from dataclasses import dataclass, field\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class SummaryState:\n",
    "    research_topic: str = field(default=None) # Report topic     \n",
    "    search_query: str = field(default=None) # Search query\n",
    "    web_research_results: Annotated[list, operator.add] = field(default_factory=list) \n",
    "    sources_gathered: Annotated[list, operator.add] = field(default_factory=list) \n",
    "    research_loop_count: int = field(default=0) # Research loop count\n",
    "    running_summary: str = field(default=None) # Final report\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class SummaryStateInput:\n",
    "    research_topic: str = field(default=None) # Report topic     \n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class SummaryStateOutput:\n",
    "    running_summary: str = field(default=None) # Final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa8c2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_thinking_tokens(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove <think> and </think> tags and their content from the text.\n",
    "    \n",
    "    Iteratively removes all occurrences of content enclosed in thinking tokens.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to process\n",
    "        \n",
    "    Returns:\n",
    "        str: The text with thinking tokens and their content removed\n",
    "    \"\"\"\n",
    "    while \"<think>\" in text and \"</think>\" in text:\n",
    "        start = text.find(\"<think>\")\n",
    "        end = text.find(\"</think>\") + len(\"</think>\")\n",
    "        text = text[:start] + text[end:]\n",
    "    return text\n",
    "\n",
    "def deduplicate_and_format_sources(\n",
    "    search_response: Union[Dict[str, Any], List[Dict[str, Any]]], \n",
    "    max_tokens_per_source: int, \n",
    "    fetch_full_page: bool = False\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Format and deduplicate search responses from various search APIs.\n",
    "    \n",
    "    Takes either a single search response or list of responses from search APIs,\n",
    "    deduplicates them by URL, and formats them into a structured string.\n",
    "    \n",
    "    Args:\n",
    "        search_response (Union[Dict[str, Any], List[Dict[str, Any]]]): Either:\n",
    "            - A dict with a 'results' key containing a list of search results\n",
    "            - A list of dicts, each containing search results\n",
    "        max_tokens_per_source (int): Maximum number of tokens to include for each source's content\n",
    "        fetch_full_page (bool, optional): Whether to include the full page content. Defaults to False.\n",
    "            \n",
    "    Returns:\n",
    "        str: Formatted string with deduplicated sources\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If input is neither a dict with 'results' key nor a list of search results\n",
    "    \"\"\"\n",
    "    # Convert input to list of results\n",
    "    if isinstance(search_response, dict):\n",
    "        sources_list = search_response['results']\n",
    "    elif isinstance(search_response, list):\n",
    "        sources_list = []\n",
    "        for response in search_response:\n",
    "            if isinstance(response, dict) and 'results' in response:\n",
    "                sources_list.extend(response['results'])\n",
    "            else:\n",
    "                sources_list.extend(response)\n",
    "    else:\n",
    "        raise ValueError(\"Input must be either a dict with 'results' or a list of search results\")\n",
    "    \n",
    "    # Deduplicate by URL\n",
    "    unique_sources = {}\n",
    "    for source in sources_list:\n",
    "        if source['url'] not in unique_sources:\n",
    "            unique_sources[source['url']] = source\n",
    "    \n",
    "    # Format output\n",
    "    formatted_text = \"Sources:\\n\\n\"\n",
    "    for i, source in enumerate(unique_sources.values(), 1):\n",
    "        formatted_text += f\"Source: {source['title']}\\n===\\n\"\n",
    "        formatted_text += f\"URL: {source['url']}\\n===\\n\"\n",
    "        formatted_text += f\"Most relevant content from source: {source['content']}\\n===\\n\"\n",
    "        if fetch_full_page:\n",
    "            # Using rough estimate of 4 characters per token\n",
    "            char_limit = max_tokens_per_source * 4\n",
    "            # Handle None raw_content\n",
    "            raw_content = source.get('raw_content', '')\n",
    "            if raw_content is None:\n",
    "                raw_content = ''\n",
    "                print(f\"Warning: No raw_content found for source {source['url']}\")\n",
    "            if len(raw_content) > char_limit:\n",
    "                raw_content = raw_content[:char_limit] + \"... [truncated]\"\n",
    "            formatted_text += f\"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\\n\\n\"\n",
    "                \n",
    "    return formatted_text.strip()\n",
    "\n",
    "def format_sources(search_results: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Format search results into a bullet-point list of sources with URLs.\n",
    "    \n",
    "    Creates a simple bulleted list of search results with title and URL for each source.\n",
    "    \n",
    "    Args:\n",
    "        search_results (Dict[str, Any]): Search response containing a 'results' key with\n",
    "                                        a list of search result objects\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted string with sources as bullet points in the format \"* title : url\"\n",
    "    \"\"\"\n",
    "    return '\\n'.join(\n",
    "        f\"* {source['title']} : {source['url']}\"\n",
    "        for source in search_results['results']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "887271be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(state: SummaryState):\n",
    "    \"\"\"LangGraph node that generates a search query based on the research topic.\n",
    "    \n",
    "    Uses an LLM to create an optimized search query for web research based on\n",
    "    the user's research topic. Supports both LMStudio and Ollama as LLM providers.\n",
    "    \n",
    "    Args:\n",
    "        state: Current graph state containing the research topic\n",
    "        config: Configuration for the runnable, including LLM provider settings\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with state update, including search_query key containing the generated query\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the prompt\n",
    "    current_date = get_current_date()\n",
    "    formatted_prompt = query_writer_instructions.format(\n",
    "        current_date=current_date,\n",
    "        research_topic=state.research_topic\n",
    "    )\n",
    "    print(formatted_prompt)\n",
    "\n",
    "    result = llm.invoke(\n",
    "        [SystemMessage(content=formatted_prompt),\n",
    "        HumanMessage(content=f\"Generate a query for web search:\")]\n",
    "    )\n",
    "    \n",
    "    # Get the content\n",
    "    content = result.content\n",
    "\n",
    "    try:\n",
    "        query = json.loads(content)\n",
    "        search_query = query['query']\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        # If parsing fails or the key is not found, use a fallback query\n",
    "        content = strip_thinking_tokens(content)\n",
    "        search_query = content\n",
    "    return {\"search_query\": search_query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "265f0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from tavily import TavilyClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a75c85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "def tavily_search(query: str, fetch_full_page: bool = True, max_results: int = 3) -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Search the web using the Tavily API and return formatted results.\n",
    "    \n",
    "    Uses the TavilyClient to perform searches. Tavily API key must be configured\n",
    "    in the environment.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query to execute\n",
    "        fetch_full_page (bool, optional): Whether to include raw content from sources.\n",
    "                                         Defaults to True.\n",
    "        max_results (int, optional): Maximum number of results to return. Defaults to 3.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, List[Dict[str, Any]]]: Search response containing:\n",
    "            - results (list): List of search result dictionaries, each containing:\n",
    "                - title (str): Title of the search result\n",
    "                - url (str): URL of the search result\n",
    "                - content (str): Snippet/summary of the content\n",
    "                - raw_content (str or None): Full content of the page if available and \n",
    "                                            fetch_full_page is True\n",
    "    \"\"\"\n",
    "     \n",
    "    tavily_client = TavilyClient()\n",
    "    return tavily_client.search(query, \n",
    "                         max_results=max_results, \n",
    "                         include_raw_content=fetch_full_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3ac706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_research(state: SummaryState):\n",
    "    search_results = tavily_search(state.search_query, fetch_full_page=True, max_results=1)\n",
    "    search_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=1000, fetch_full_page=True)\n",
    "    return {\"sources_gathered\": [format_sources(search_results)], \"research_loop_count\": state.research_loop_count + 1, \"web_research_results\": [search_str]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49c4d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_sources(state: SummaryState):\n",
    "    \"\"\"LangGraph node that summarizes web research results.\n",
    "    \n",
    "    Uses an LLM to create or update a running summary based on the newest web research \n",
    "    results, integrating them with any existing summary.\n",
    "    \n",
    "    Args:\n",
    "        state: Current graph state containing research topic, running summary,\n",
    "              and web research results\n",
    "        config: Configuration for the runnable, including LLM provider settings\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with state update, including running_summary key containing the updated summary\n",
    "    \"\"\"\n",
    "\n",
    "    # Existing summary\n",
    "    existing_summary = state.running_summary\n",
    "\n",
    "    # Most recent web research\n",
    "    most_recent_web_research = state.web_research_results[-1]\n",
    "\n",
    "    # Build the human message\n",
    "    if existing_summary:\n",
    "        human_message_content = (\n",
    "            f\"<Existing Summary> \\n {existing_summary} \\n <Existing Summary>\\n\\n\"\n",
    "            f\"<New Context> \\n {most_recent_web_research} \\n <New Context>\"\n",
    "            f\"Update the Existing Summary with the New Context on this topic: \\n <User Input> \\n {state.research_topic} \\n <User Input>\\n\\n\"\n",
    "        )\n",
    "    else:\n",
    "        human_message_content = (\n",
    "            f\"<Context> \\n {most_recent_web_research} \\n <Context>\"\n",
    "            f\"Create a Summary using the Context on this topic: \\n <User Input> \\n {state.research_topic} \\n <User Input>\\n\\n\"\n",
    "        )\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-2024-08-06\", temperature=0)\n",
    "\n",
    "    result = llm.invoke(\n",
    "        [SystemMessage(content=summarizer_instructions),\n",
    "        HumanMessage(content=human_message_content)]\n",
    "    )\n",
    "\n",
    "    # Strip thinking tokens if configured\n",
    "    running_summary = result.content\n",
    "    running_summary = strip_thinking_tokens(running_summary)\n",
    "\n",
    "    return {\"running_summary\": running_summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91a427f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflect_on_summary(state: SummaryState):\n",
    "    \"\"\"LangGraph node that identifies knowledge gaps and generates follow-up queries.\n",
    "    \n",
    "    Analyzes the current summary to identify areas for further research and generates\n",
    "    a new search query to address those gaps. Uses structured output to extract\n",
    "    the follow-up query in JSON format.\n",
    "    \n",
    "    Args:\n",
    "        state: Current graph state containing the running summary and research topic\n",
    "        config: Configuration for the runnable, including LLM provider settings\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with state update, including search_query key containing the generated follow-up query\n",
    "    \"\"\"\n",
    "    # Choose the appropriate LLM based on the provider\n",
    "    llm = ChatLMStudio(format=\"json\")\n",
    "    \n",
    "    result = llm.invoke(\n",
    "        [SystemMessage(content=reflection_instructions.format(research_topic=state.research_topic)),\n",
    "        HumanMessage(content=f\"Reflect on our existing knowledge: \\n === \\n {state.running_summary}, \\n === \\n And now identify a knowledge gap and generate a follow-up web search query:\")]\n",
    "    )\n",
    "    \n",
    "    # Strip thinking tokens if configured\n",
    "    try:\n",
    "        # Try to parse as JSON first\n",
    "        reflection_content = json.loads(result.content)\n",
    "        # Get the follow-up query\n",
    "        query = reflection_content.get('follow_up_query')\n",
    "        # Check if query is None or empty\n",
    "        if not query:\n",
    "            # Use a fallback query\n",
    "            return {\"search_query\": f\"Tell me more about {state.research_topic}\"}\n",
    "        return {\"search_query\": query}\n",
    "    except (json.JSONDecodeError, KeyError, AttributeError):\n",
    "        # If parsing fails or the key is not found, use a fallback query\n",
    "        return {\"search_query\": f\"Tell me more about {state.research_topic}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81057ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_summary(state: SummaryState):\n",
    "    \"\"\"LangGraph node that finalizes the research summary.\n",
    "    \n",
    "    Prepares the final output by deduplicating and formatting sources, then\n",
    "    combining them with the running summary to create a well-structured\n",
    "    research report with proper citations.\n",
    "    \n",
    "    Args:\n",
    "        state: Current graph state containing the running summary and sources gathered\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with state update, including running_summary key containing the formatted final summary with sources\n",
    "    \"\"\"\n",
    "\n",
    "    # Deduplicate sources before joining\n",
    "    seen_sources = set()\n",
    "    unique_sources = []\n",
    "    \n",
    "    for source in state.sources_gathered:\n",
    "        # Split the source into lines and process each individually\n",
    "        for line in source.split('\\n'):\n",
    "            # Only process non-empty lines\n",
    "            if line.strip() and line not in seen_sources:\n",
    "                seen_sources.add(line)\n",
    "                unique_sources.append(line)\n",
    "    \n",
    "    # Join the deduplicated sources\n",
    "    all_sources = \"\\n\".join(unique_sources)\n",
    "    state.running_summary = f\"## Summary\\n{state.running_summary}\\n\\n ### Sources:\\n{all_sources}\"\n",
    "    return {\"running_summary\": state.running_summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12b1fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional, Literal\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "def route_research(state: SummaryState) -> Literal[\"finalize_summary\", \"web_research\"]:\n",
    "    \"\"\"LangGraph routing function that determines the next step in the research flow.\n",
    "    \n",
    "    Controls the research loop by deciding whether to continue gathering information\n",
    "    or to finalize the summary based on the configured maximum number of research loops.\n",
    "    \n",
    "    Args:\n",
    "        state: Current graph state containing the research loop count\n",
    "        config: Configuration for the runnable, including max_web_research_loops setting\n",
    "        \n",
    "    Returns:\n",
    "        String literal indicating the next node to visit (\"web_research\" or \"finalize_summary\")\n",
    "    \"\"\"\n",
    "\n",
    "    if state.research_loop_count <= 3:\n",
    "        return \"web_research\"\n",
    "    else:\n",
    "        return \"finalize_summary\"\n",
    "\n",
    "# Add nodes and edges\n",
    "builder = StateGraph(SummaryState, input=SummaryStateInput, output=SummaryStateOutput)\n",
    "builder.add_node(\"generate_query\", generate_query)\n",
    "builder.add_node(\"web_research\", web_research)\n",
    "builder.add_node(\"summarize_sources\", summarize_sources)\n",
    "builder.add_node(\"reflect_on_summary\", reflect_on_summary)\n",
    "builder.add_node(\"finalize_summary\", finalize_summary)\n",
    "\n",
    "# Add edges\n",
    "builder.add_edge(START, \"generate_query\")\n",
    "builder.add_edge(\"generate_query\", \"web_research\")\n",
    "builder.add_edge(\"web_research\", \"summarize_sources\")\n",
    "builder.add_edge(\"summarize_sources\", \"reflect_on_summary\")\n",
    "builder.add_conditional_edges(\"reflect_on_summary\", route_research)\n",
    "builder.add_edge(\"finalize_summary\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba0ae1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your goal is to generate a targeted web search query.\n",
      "\n",
      "<CONTEXT>\n",
      "Current date: April 08, 2025\n",
      "Please ensure your queries account for the most current information available as of this date.\n",
      "</CONTEXT>\n",
      "\n",
      "<TOPIC>\n",
      "phi-4에 대해서 알려줘\n",
      "</TOPIC>\n",
      "\n",
      "<FORMAT>\n",
      "Format your response as a JSON object with ALL three of these exact keys:\n",
      "   - \"query\": The actual search query string\n",
      "   - \"rationale\": Brief explanation of why this query is relevant\n",
      "</FORMAT>\n",
      "\n",
      "<EXAMPLE>\n",
      "Example output:\n",
      "{\n",
      "    \"query\": \"machine learning transformer architecture explained\",\n",
      "    \"rationale\": \"Understanding the fundamental structure of transformer models\"\n",
      "}\n",
      "</EXAMPLE>\n",
      "\n",
      "Provide your response in JSON format:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing JSON response: 'ChatGeneration' object is not subscriptable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate_query': {'search_query': 'phi-4 theory explained 2025'}}\n",
      "{'web_research': {'sources_gathered': ['* Microsoft Phi-4 is a Small Language Model Specialized for ... - InfoQ : https://www.infoq.com/news/2025/01/microsoft-phi-4/'], 'research_loop_count': 1, 'web_research_results': [\"Sources:\\n\\nSource: Microsoft Phi-4 is a Small Language Model Specialized for ... - InfoQ\\n===\\nURL: https://www.infoq.com/news/2025/01/microsoft-phi-4/\\n===\\nMost relevant content from source: Phi-4 is 14B parameter model from Microsoft Research that aims to improve the state of the art for math reasoning. Previously available on Azure AI Foundry, Phi-4 has recently become available on\\n===\\nFull source content limited to 1000 tokens: InfoQ Software Architects' Newsletter\\n\\nA monthly overview of things you need to know as an architect or aspiring architect.\\n\\nView an example\\n\\n\\n\\nWe protect your privacy.\\n\\nInfoQ Dev Summit Munich (October 15-16): Deep-dive into 25+ technical talks from senior developers sharing real-world projects.\\n\\n                            Register Now\\n\\nFacilitating the Spread of Knowledge and Innovation in Professional Software Development\\n\\n\\n\\n\\n\\nBack to login\\n\\n\\n\\nBack to login\\n\\nLogin with:\\n\\nDon't have an InfoQ account?\\n\\nTopics\\n\\nFeatured in  Development\\n\\nBuilding Inclusive Mini Golf: a Practical Guide to Accessible XR Development\\n\\nColby Morgan discusses practical strategies and technical examples for building accessible and inclusive XR experiences. Learn about their core design principles, including accessibility at the start, invisible features, simplicity, and layered depth, using Walkabout Mini Golf as a case study.\\n\\nFeatured in  Architecture & Design\\n\\nBalancing Coupling in Software Design with Vlad Khononov\\n\\nIn this episode, Thomas Betts speaks with Vlad Khononov about balancing coupling in software design, the subject of his recent book. They discuss how coupling is necessary for a system to function, but has to be balanced to allow the system to evolve. Vlad identifies three factors that can be used to measure coupling: knowledge sharing, distance, and volatility.\\n\\nFeatured in  AI, ML & Data Engineering\\n\\nUnleashing Llama's Potential: CPU-based Fine-tuning\\n\\nAnil Rajput and Rema Hariharan discuss the crucial role of CPU architecture in optimizing Large Language Model (LLM), specifically Llama, performance. They explain hardware-software synchronization for TCO reduction and latency improvements. Learn about core utilization, cache impact, memory bandwidth considerations, and the benefits of chiplet architecture for LLM deployments on CPUs.\\n\\nFeatured in  Culture & Methods\\n\\nBuilding Your Personal Brand and Making an Impact: Insights from Principal Engineer Pablo Fredrikson\\n\\nIn this podcast, Shane Hastie, Lead Editor for Culture & Methods, spoke to Pablo Fredrikson, a principal engineer at Bitso, about the importance of building a personal brand, sharing knowledge, and helping others in the tech industry.\\n\\nFeatured in  DevOps\\n\\nChecklist for Kubernetes in Production: Best Practices for SREs\\n\\nThis article provides SREs with a checklist for managing Kubernetes in production environments. It identifies common challenges including resource management, workload placement, high availability, health probes, storage, monitoring, and cost optimization. By implementing consistent GitOps automation across these areas, teams can significantly reduce complexity, and prevent downtime.\\n\\nHelpful links\\n\\nChoose your language\\n\\nLearn how senior software developers are solving the challenges you face. Register now with early bird tickets.\\n\\nLearn practical solutions to today's most pressing software challenges. Register now with early bird tickets.\\n\\nExplore insights, real-world best practices and solutions in software development & leadership. Register now.\\n\\nLearn how leading engineering teams run AI in production-reliably, securely, and at scale. Register now.\\n\\nInfoQ Homepage\\nNews\\nMicrosoft Phi-4 is a Small Language Model Specialized for Complex Math Reasoning\\n\\nMicrosoft Phi-4 is a Small Language Model Specialized for Complex Math Reasoning\\n\\nJan 24, 2025\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t2\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tmin read\\n\\nby\\n\\nSergio De Simone\\n\\nWrite for InfoQ\\n\\nPhi-4 is 14B parameter model from Microsoft Research that aims to improve the state of the art for math reasoning. Previously available on Azure AI Foundry, Phi-4 has recently become available on Hugging Face under the MIT license.\\n\\nAccording to Microsoft, Phi-4 outperforms comparable and larger models on math reasoning thanks to a number of innovations throughout the training process, including the use of synthetic data for pre-training and mid-training, curation and filtering of organic data, and a new post... [truncated]\"]}}\n",
      "{'summarize_sources': {'running_summary': 'Microsoft Phi-4 is a 14 billion parameter language model developed by Microsoft Research, designed to enhance mathematical reasoning capabilities. Initially available on Azure AI Foundry, it has now been released on Hugging Face under the MIT license. Phi-4 distinguishes itself by outperforming both comparable and larger models in math reasoning tasks. This is achieved through several innovative approaches during its training process, including the use of synthetic data for pre-training and mid-training, as well as the careful curation and filtering of organic data. These strategies contribute to its superior performance in handling complex mathematical problems.'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing JSON response: 'ChatGeneration' object is not subscriptable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reflect_on_summary': {'search_query': \"What specific techniques and types of synthetic data were used in the training of Microsoft's Phi-4 model, and how were the organic data curated and filtered to improve its mathematical reasoning capabilities?\"}}\n",
      "{'web_research': {'sources_gathered': ['* microsoft/phi-4 - Hugging Face : https://huggingface.co/microsoft/phi-4'], 'research_loop_count': 2, 'web_research_results': ['Sources:\\n\\nSource: microsoft/phi-4 - Hugging Face\\n===\\nURL: https://huggingface.co/microsoft/phi-4\\n===\\nMost relevant content from source: Phi-4 Model Card | Description | phi-4 is a state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets. The goal of this approach was to ensure that small capable models were trained with data focused on high quality and advanced reasoning. 1. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. model=\"microsoft/phi-4\", Limited Scope for Code: Majority of phi-4 training data is based in Python and uses common packages such as typing, math, random, collections, datetime, itertools. Model tree for microsoft/phi-4\\n===\\nFull source content limited to 1000 tokens: microsoft/phi-4 · Hugging Face\\n Hugging Face\\n\\nModels\\nDatasets\\nSpaces\\nPosts\\nDocs\\nEnterprise\\n\\nPricing\\n\\n\\n\\n\\n\\nLog In\\n\\nSign Up\\n\\n\\nmicrosoft\\n/\\nphi-4\\nlike 1.96k\\nFollow\\n Microsoft 10.8k\\n================================================================================================================================\\nText GenerationTransformersSafetensorsEnglishphi3phinlpmathcodechatconversationaltext-generation-inference\\narxiv: 2412.08905\\nLicense: mit\\nModel card Files Files and versions Community 42\\nTrain\\nDeploy\\nUse this model\\n\\n\\nPhi-4 Model Card\\n\\n\\nModel Summary\\n\\n\\nIntended Use\\n\\n\\nData Overview\\n\\nTraining Datasets\\n\\n\\nSafety\\nApproach\\nSafety Evaluation and Red-Teaming\\n\\n\\n\\nModel Quality\\n\\n\\nUsage\\n\\nInput Formats\\nWith transformers\\n\\n\\nResponsible AI Considerations\\n\\n\\n\\nPhi-4 Model Card\\nPhi-4 Technical Report\\nModel Summary\\n|  |  |\\n| --- | --- |\\n| Developers | Microsoft Research |\\n| Description | phi-4 is a state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets. The goal of this approach was to ensure that small capable models were trained with data focused on high quality and advanced reasoning.  \\nphi-4 underwent a rigorous enhancement and alignment process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures |\\n| Architecture | 14B parameters, dense decoder-only Transformer model |\\n| Inputs | Text, best suited for prompts in the chat format |\\n| Context length | 16K tokens |\\n| GPUs | 1920 H100-80G |\\n| Training time | 21 days |\\n| Training data | 9.8T tokens |\\n| Outputs | Generated text in response to input |\\n| Dates | October 2024 – November 2024 |\\n| Status | Static model trained on an offline dataset with cutoff dates of June 2024 and earlier for publicly available data |\\n| Release date | December 12, 2024 |\\n| License | MIT |\\nIntended Use\\n|  |  |\\n| --- | --- |\\n| Primary Use Cases | Our model is designed to accelerate research on language models, for use as a building block for generative AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) which require:  \\n1. Memory/compute constrained environments.\\n2. Latency bound scenarios.\\n3. Reasoning and logic. |\\n| Out-of-Scope Use Cases | Our models is not specifically designed or evaluated for all downstream purposes, thus:  \\n1. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.\\n2. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case, including the model’s focus on English.\\n3. Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under. |\\nData Overview\\nTraining Datasets\\nOur training data is an extension of the data used for Phi-3 and includes a wide variety of sources from:\\n\\n\\nPublicly available documents filtered rigorously for quality, selected high-quality educational data, and code.\\n\\n\\nNewly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.).\\n\\n\\nAcquired academic books and Q&A datasets.\\n\\n\\nHigh quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\\n\\n\\nMultilingual data constitutes about 8% of our overall data. We are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge.\\nBenchmark datasets\\nWe evaluated phi-4 us... [truncated]']}}\n",
      "{'summarize_sources': {'running_summary': \"Microsoft Phi-4 is a 14 billion parameter language model developed by Microsoft Research, designed to enhance mathematical reasoning capabilities. Initially available on Azure AI Foundry, it has now been released on Hugging Face under the MIT license. Phi-4 distinguishes itself by outperforming both comparable and larger models in math reasoning tasks. This is achieved through several innovative approaches during its training process, including the use of synthetic data for pre-training and mid-training, as well as the careful curation and filtering of organic data. These strategies contribute to its superior performance in handling complex mathematical problems.\\n\\nPhi-4 is built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets. This approach ensures that the model is trained with high-quality data focused on advanced reasoning. The model underwent a rigorous enhancement and alignment process, incorporating supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures. It is a dense decoder-only Transformer model with a context length of 16K tokens, trained over 21 days using 9.8 trillion tokens. The model is designed for general-purpose AI systems and applications, particularly in memory/compute constrained environments and latency-bound scenarios. However, developers are advised to consider common limitations of language models and evaluate for accuracy, safety, and fairness, especially in high-risk scenarios. Phi-4's training data includes a significant portion of Python-based code, utilizing common packages, and it supports multilingual data, which constitutes about 8% of the overall data.\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing JSON response: 'ChatGeneration' object is not subscriptable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reflect_on_summary': {'search_query': 'What performance metrics and benchmarks are used to evaluate the mathematical reasoning capabilities of Microsoft Phi-4, and how do these metrics compare to other language models in similar tasks?'}}\n",
      "{'web_research': {'sources_gathered': ['* Microsoft phi-4: The best smallest LLM - Medium : https://medium.com/data-science-in-your-pocket/microsoft-phi-4-the-best-smallest-llm-1cbaa5706e9e'], 'research_loop_count': 3, 'web_research_results': ['Sources:\\n\\nSource: Microsoft phi-4: The best smallest LLM - Medium\\n===\\nURL: https://medium.com/data-science-in-your-pocket/microsoft-phi-4-the-best-smallest-llm-1cbaa5706e9e\\n===\\nMost relevant content from source: Microsoft phi-4: The best smallest LLM | by Mehul Gupta | Data Science in your pocket | Jan, 2025 | Medium Hailed as the best small-sized LLM, the model looks great and significantly improves over Microsoft phi-3, the previous version of Phi. Key Features The training dataset for Phi-4 builds on the data used for Phi-3, combining rigorously filtered publicly available documents, high-quality educational data, and code. Phi-4 achieves a 56.1 score, significantly higher than other 14B models like phi-3, Qwen 2.5 , and GPT-4o-mini Phi-4 consistently scores higher across key categories like science, math, code generation, and reasoning, making it the best-performing small LLM in this comparison. model=\"microsoft/phi-4\",\\n===\\nFull source content limited to 1000 tokens: Microsoft phi-4: The best smallest LLM | by Mehul Gupta | Data Science in your pocket | Jan, 2025 | Medium\\nOpen in app\\nSign up\\nSign in\\n\\nWrite\\n\\nSign up\\nSign in\\n\\nMicrosoft phi-4: The best smallest LLM\\nMicrosoft released phi-4\\n\\n\\nMehul Gupta\\n·Follow\\nPublished in\\nData Science in your pocket\\n·\\n3 min read\\n·\\n3 days ago\\n\\n--\\n1\\n\\nListen\\nShare\\n\\nPhoto by Surface on Unsplash\\nAnd the 1st LLM in 2025 is released and Microsoft phi-4 is out. Hailed as the best small-sized LLM, the model looks great and significantly improves over Microsoft phi-3, the previous version of Phi.\\nKey Features\\n\\nDeveloped by Microsoft Research: A cutting-edge open model designed by Microsoft Research.\\nHigh-Quality Training Data: Trained on synthetic datasets, filtered public domain websites, and academic books/Q&A datasets.\\nRigorous Enhancement and Alignment: Employs supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) for precise alignment and safety.\\nAdvanced Architecture: 14-billion parameter dense decoder-only Transformer model.\\nExtended Context Length: Supports a 16K token context window, ideal for long-text processing.\\nEfficient Training Process: Trained on 9.8 trillion tokens over 21 days using 1920 H100–80G GPUs.\\nVersatile Input and Output Capabilities: Accepts text inputs and generates high-quality text outputs.\\n\\nTraining Dataset\\nThe training dataset for Phi-4 builds on the data used for Phi-3, combining rigorously filtered publicly available documents, high-quality educational data, and code. It also includes newly created synthetic, “textbook-like” data for teaching math, coding, reasoning, and general knowledge, alongside acquired academic books and Q&A datasets. Additionally, the dataset incorporates high-quality chat format data designed to reflect human preferences in areas like instruction-following, truthfulness, honesty, and helpfulness, ensuring a diverse and robust foundation for training.\\nPerformance and metrics\\nThe table shows the performance of several language models (LLMs) on various benchmarks. Here’s why phi-4 (14B) is the best among the small LLMs (14B parameters or less):\\n\\n\\nPopular Aggregated Benchmark (MMLU):\\nPhi-4 scores 84.8, outperforming phi-3 , Qwen 2.5 , and GPT-4o-mini .\\nScience (GPQA):\\nPhi-4 achieves a 56.1 score, significantly higher than other 14B models like phi-3, Qwen 2.5 , and GPT-4o-mini\\nMath (MGSM and MATH):\\nMGSM: Phi-4 scores 80.6, much better than phi-3 and comparable to Llama-2–70B .\\nMATH: Phi-4 scores 80.4, far ahead of phi-3 and GPT-4o-mini.\\nCode Generation (HumanEval):\\nPhi-4 scores 82.6, leading phi-3 and Qwen 2.5 .\\nReasoning (DROP):\\nPhi-4 achieves 75.5, higher than phi-3 and GPT-4o-mini.\\nFactual Knowledge (SimpleQA):\\nAlthough low, phi-4’s score of 3.0 is comparable to other small LLMs like Qwen 2.5 .\\n\\nPhi-4 consistently scores higher across key categories like science, math, code generation, and reasoning, making it the best-performing small LLM in this comparison.\\nHow to use the model?\\nUsing transformers\\nimport transformers\\nimport os  \\nos.environ[\"HF_TOKEN\"] = \\'\\'  \\npipeline = transformers.pipeline(\\n    \"text-generation\",\\n    model=\"microsoft/phi-4\",\\n    model_kwargs={\"torch_dtype\": \"auto\"},\\n    device_map=\"auto\",\\n)  \\nmessages = [\\n    {\"role\": \"system\", \"content\": \"You\\'re a kid. Explain in simple terms\"},\\n    {\"role\": \"user\", \"content\": \"How should I explain the Internet?\"},\\n]  \\noutputs = pipeline(messages, max_new_tokens=128)\\nprint(outputs[0][\"generated_text\"][-1])\\nIf you don\\'t have enough hardware, try it on Huggingface Spaces\\nPhi-4 - a Hugging Face Space by eswardivi ----------------------------------------- ### Chat with LLMs huggingface.co\\n\\nI hope you try out the model !\\n\\n\\nSign up to discover human stories that deepen your understanding of the world.\\nFree\\nDistraction-free reading. No ads.\\nOrganize your knowledge with lists and highlights.\\nTell your story. Find your audience.\\nSign up for free\\nMembership\\nRead member-only stories\\nSupport writers you read most\\nEarn money for your writing\\nListen to a... [truncated]']}}\n",
      "{'summarize_sources': {'running_summary': 'Microsoft Phi-4 is a 14 billion parameter language model developed by Microsoft Research, designed to enhance mathematical reasoning capabilities. Initially available on Azure AI Foundry, it has now been released on Hugging Face under the MIT license. Phi-4 distinguishes itself by outperforming both comparable and larger models in math reasoning tasks. This is achieved through several innovative approaches during its training process, including the use of synthetic data for pre-training and mid-training, as well as the careful curation and filtering of organic data. These strategies contribute to its superior performance in handling complex mathematical problems.\\n\\nPhi-4 is built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets. This approach ensures that the model is trained with high-quality data focused on advanced reasoning. The model underwent a rigorous enhancement and alignment process, incorporating supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures. It is a dense decoder-only Transformer model with a context length of 16K tokens, trained over 21 days using 9.8 trillion tokens. The model is designed for general-purpose AI systems and applications, particularly in memory/compute constrained environments and latency-bound scenarios. However, developers are advised to consider common limitations of language models and evaluate for accuracy, safety, and fairness, especially in high-risk scenarios. Phi-4\\'s training data includes a significant portion of Python-based code, utilizing common packages, and it supports multilingual data, which constitutes about 8% of the overall data.\\n\\nPhi-4 is hailed as the best small-sized LLM, significantly improving over its predecessor, Phi-3. It achieves a 56.1 score in science, math, code generation, and reasoning, outperforming other 14B models like phi-3, Qwen 2.5, and GPT-4o-mini. The training dataset for Phi-4 builds on the data used for Phi-3, combining rigorously filtered publicly available documents, high-quality educational data, and code. It also includes newly created synthetic, \"textbook-like\" data for teaching math, coding, reasoning, and general knowledge, alongside high-quality chat format data designed to reflect human preferences in areas like instruction-following, truthfulness, honesty, and helpfulness. Phi-4 consistently scores higher across key categories, making it the best-performing small LLM in its class.'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing JSON response: 'ChatGeneration' object is not subscriptable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reflect_on_summary': {'search_query': \"What are the specific innovative techniques employed in the training of Microsoft's Phi-4 model, especially concerning the creation and use of synthetic data, and how do these techniques differ from those used in other contemporary language models?\"}}\n",
      "{'web_research': {'sources_gathered': ['* Phi-4 Technical Report - Microsoft Research : https://www.microsoft.com/en-us/research/publication/phi-4-technical-report/'], 'research_loop_count': 4, 'web_research_results': ['Sources:\\n\\nSource: Phi-4 Technical Report - Microsoft Research\\n===\\nURL: https://www.microsoft.com/en-us/research/publication/phi-4-technical-report/\\n===\\nMost relevant content from source: We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality.\\n===\\nFull source content limited to 1000 tokens: Global\\n\\nPhi-4 Technical Report\\n\\nMSR-TR-2024-57 | December 2024\\n\\nPublished by Microsoft\\n\\nPDF | Related File\\n\\nWe present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size– especially on reasoning-focused benchmarks– due to improved data, training curriculum, and innovations in the post-training scheme.\\n\\nResearch Areas\\n\\nResearch Labs\\n\\nFollow us:\\n\\nShare this page:']}}\n",
      "{'summarize_sources': {'running_summary': 'Microsoft Phi-4 is a 14 billion parameter language model developed by Microsoft Research, designed to enhance mathematical reasoning capabilities. Initially available on Azure AI Foundry, it has now been released on Hugging Face under the MIT license. Phi-4 distinguishes itself by outperforming both comparable and larger models in math reasoning tasks. This is achieved through several innovative approaches during its training process, including the use of synthetic data for pre-training and mid-training, as well as the careful curation and filtering of organic data. These strategies contribute to its superior performance in handling complex mathematical problems.\\n\\nPhi-4 is built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets. This approach ensures that the model is trained with high-quality data focused on advanced reasoning. The model underwent a rigorous enhancement and alignment process, incorporating supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures. It is a dense decoder-only Transformer model with a context length of 16K tokens, trained over 21 days using 9.8 trillion tokens. The model is designed for general-purpose AI systems and applications, particularly in memory/compute constrained environments and latency-bound scenarios. However, developers are advised to consider common limitations of language models and evaluate for accuracy, safety, and fairness, especially in high-risk scenarios. Phi-4\\'s training data includes a significant portion of Python-based code, utilizing common packages, and it supports multilingual data, which constitutes about 8% of the overall data.\\n\\nPhi-4 is hailed as the best small-sized LLM, significantly improving over its predecessor, Phi-3. It achieves a 56.1 score in science, math, code generation, and reasoning, outperforming other 14B models like phi-3, Qwen 2.5, and GPT-4o-mini. The training dataset for Phi-4 builds on the data used for Phi-3, combining rigorously filtered publicly available documents, high-quality educational data, and code. It also includes newly created synthetic, \"textbook-like\" data for teaching math, coding, reasoning, and general knowledge, alongside high-quality chat format data designed to reflect human preferences in areas like instruction-following, truthfulness, honesty, and helpfulness. Phi-4 consistently scores higher across key categories, making it the best-performing small LLM in its class.\\n\\nThe Phi-4 model\\'s development was centrally focused on data quality, strategically incorporating synthetic data throughout the training process. Unlike previous models in the Phi family, which largely distilled capabilities from a teacher model (specifically GPT-4), Phi-4 surpasses its teacher model in STEM-focused QA capabilities. This demonstrates that its data-generation and post-training techniques extend beyond simple distillation, achieving strong performance on reasoning-focused benchmarks due to improved data, training curriculum, and innovations in the post-training scheme.'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing JSON response: 'ChatGeneration' object is not subscriptable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reflect_on_summary': {'search_query': 'What are the specific synthetic data generation techniques used in training the Microsoft Phi-4 model, and how do they enhance its mathematical reasoning capabilities?'}}\n",
      "{'finalize_summary': {'running_summary': '## Summary\\nMicrosoft Phi-4 is a 14 billion parameter language model developed by Microsoft Research, designed to enhance mathematical reasoning capabilities. Initially available on Azure AI Foundry, it has now been released on Hugging Face under the MIT license. Phi-4 distinguishes itself by outperforming both comparable and larger models in math reasoning tasks. This is achieved through several innovative approaches during its training process, including the use of synthetic data for pre-training and mid-training, as well as the careful curation and filtering of organic data. These strategies contribute to its superior performance in handling complex mathematical problems.\\n\\nPhi-4 is built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets. This approach ensures that the model is trained with high-quality data focused on advanced reasoning. The model underwent a rigorous enhancement and alignment process, incorporating supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures. It is a dense decoder-only Transformer model with a context length of 16K tokens, trained over 21 days using 9.8 trillion tokens. The model is designed for general-purpose AI systems and applications, particularly in memory/compute constrained environments and latency-bound scenarios. However, developers are advised to consider common limitations of language models and evaluate for accuracy, safety, and fairness, especially in high-risk scenarios. Phi-4\\'s training data includes a significant portion of Python-based code, utilizing common packages, and it supports multilingual data, which constitutes about 8% of the overall data.\\n\\nPhi-4 is hailed as the best small-sized LLM, significantly improving over its predecessor, Phi-3. It achieves a 56.1 score in science, math, code generation, and reasoning, outperforming other 14B models like phi-3, Qwen 2.5, and GPT-4o-mini. The training dataset for Phi-4 builds on the data used for Phi-3, combining rigorously filtered publicly available documents, high-quality educational data, and code. It also includes newly created synthetic, \"textbook-like\" data for teaching math, coding, reasoning, and general knowledge, alongside high-quality chat format data designed to reflect human preferences in areas like instruction-following, truthfulness, honesty, and helpfulness. Phi-4 consistently scores higher across key categories, making it the best-performing small LLM in its class.\\n\\nThe Phi-4 model\\'s development was centrally focused on data quality, strategically incorporating synthetic data throughout the training process. Unlike previous models in the Phi family, which largely distilled capabilities from a teacher model (specifically GPT-4), Phi-4 surpasses its teacher model in STEM-focused QA capabilities. This demonstrates that its data-generation and post-training techniques extend beyond simple distillation, achieving strong performance on reasoning-focused benchmarks due to improved data, training curriculum, and innovations in the post-training scheme.\\n\\n ### Sources:\\n* Microsoft Phi-4 is a Small Language Model Specialized for ... - InfoQ : https://www.infoq.com/news/2025/01/microsoft-phi-4/\\n* microsoft/phi-4 - Hugging Face : https://huggingface.co/microsoft/phi-4\\n* Microsoft phi-4: The best smallest LLM - Medium : https://medium.com/data-science-in-your-pocket/microsoft-phi-4-the-best-smallest-llm-1cbaa5706e9e\\n* Phi-4 Technical Report - Microsoft Research : https://www.microsoft.com/en-us/research/publication/phi-4-technical-report/'}}\n"
     ]
    }
   ],
   "source": [
    "for i in graph.stream({\"research_topic\":\"phi-4에 대해서 알려줘\"}):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0bce66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
